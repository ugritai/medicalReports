{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesar datos hosp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, los archivos .csv comprimidos en la carpeta hosp son descomprimidos y asignados a 4 subconjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de hosp...\n",
      "Procesando: poe (subconjunto1)\n",
      "Guardado: subconjunto1_poe.csv (52212109 filas)\n",
      "Procesando: d_icd_procedures (subconjunto1)\n",
      "Guardado: subconjunto1_d_icd_procedures.csv (86423 filas)\n",
      "Procesando: poe_detail (subconjunto1)\n",
      "Guardado: subconjunto1_poe_detail.csv (8504982 filas)\n",
      "Procesando: prescriptions (subconjunto1)\n",
      "Guardado: subconjunto1_prescriptions.csv (20292611 filas)\n",
      "Procesando: omr (subconjunto1)\n",
      "Guardado: subconjunto1_omr.csv (7753027 filas)\n",
      "Procesando: services (subconjunto2)\n",
      "Guardado: subconjunto2_services.csv (593071 filas)\n",
      "Procesando: pharmacy (subconjunto2)\n",
      "Guardado: subconjunto2_pharmacy.csv (17847567 filas)\n",
      "Procesando: d_icd_diagnoses (subconjunto2)\n",
      "Guardado: subconjunto2_d_icd_diagnoses.csv (112107 filas)\n",
      "Procesando: diagnoses_icd (subconjunto2)\n",
      "Guardado: subconjunto2_diagnoses_icd.csv (6364488 filas)\n",
      "Procesando: emar_detail (subconjunto2)\n",
      "Guardado: subconjunto2_emar_detail.csv (87371064 filas)\n",
      "Procesando: labevents (subconjunto3)\n",
      "Guardado: subconjunto3_labevents.csv (158374764 filas)\n",
      "Procesando: d_labitems (subconjunto3)\n",
      "Guardado: subconjunto3_d_labitems.csv (1650 filas)\n",
      "Procesando: drgcodes (subconjunto3)\n",
      "Guardado: subconjunto3_drgcodes.csv (761856 filas)\n",
      "Procesando: emar (subconjunto3)\n",
      "Guardado: subconjunto3_emar.csv (42808593 filas)\n",
      "Procesando: microbiologyevents (subconjunto3)\n",
      "Guardado: subconjunto3_microbiologyevents.csv (3988224 filas)\n",
      "Procesando: transfers (subconjunto4)\n",
      "Guardado: subconjunto4_transfers.csv (2413581 filas)\n",
      "Procesando: d_hcpcs (subconjunto4)\n",
      "Guardado: subconjunto4_d_hcpcs.csv (89208 filas)\n",
      "Procesando: provider (subconjunto4)\n",
      "Guardado: subconjunto4_provider.csv (42244 filas)\n",
      "Procesando: procedures_icd (subconjunto4)\n",
      "Guardado: subconjunto4_procedures_icd.csv (859655 filas)\n",
      "Procesando: hcpcsevents (subconjunto4)\n",
      "Guardado: subconjunto4_hcpcsevents.csv (186074 filas)\n",
      "Procesando: admissions (subconjunto4)\n",
      "Guardado: subconjunto4_admissions.csv (546028 filas)\n",
      "Procesando: patients (subconjunto4)\n",
      "Guardado: subconjunto4_patients.csv (364627 filas)\n"
     ]
    }
   ],
   "source": [
    "# CARGAR DATOS EN SUBCONJUNTOS\n",
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_principal = r\"/home/raulmartinez/medicalReports/mimic-iv-3.1\"\n",
    "\n",
    "# Función para leer archivos y dividirlo en cuatro subconjuntos\n",
    "def cargar_y_dividir_datos(ruta_modulo):\n",
    "    archivos = [archivo for archivo in os.listdir(ruta_modulo) if archivo.endswith(\".gz\")]\n",
    "    total_archivos = len(archivos)\n",
    "    tam_subconjunto = total_archivos // 4\n",
    "\n",
    "    # División del conjunto hosp en 4 subconjuntos\n",
    "    for i, archivo in enumerate(archivos):\n",
    "        ruta_gz = os.path.join(ruta_modulo, archivo)\n",
    "        nombre_tabla = archivo.replace(\".csv.gz\", \"\")  # Nombre de la tabla sin la extensión\n",
    "\n",
    "        if i < tam_subconjunto:\n",
    "            parte = \"subconjunto1\"\n",
    "        elif i < 2*tam_subconjunto:\n",
    "            parte = \"subconjunto2\"\n",
    "        elif i < 3*tam_subconjunto:\n",
    "            parte = \"subconjunto3\"\n",
    "        else:\n",
    "            parte = \"subconjunto4\"\n",
    "        \n",
    "        print(f\"Procesando: {nombre_tabla} ({parte})\")\n",
    "        \n",
    "        # Leer el archivo comprimido\n",
    "        df = pd.read_csv(ruta_gz, compression=\"gzip\", low_memory=False)\n",
    "        # Guardar en un .csv nuevo\n",
    "        df.to_csv(f\"datasets/{parte}_{nombre_tabla}.csv\", index=False)\n",
    "        print(f\"Guardado: {parte}_{nombre_tabla}.csv ({df.shape[0]} filas)\")\n",
    "\n",
    "# Cargar datos de hosp\n",
    "ruta_hosp = os.path.join(ruta_principal, \"hosp\")\n",
    "print (\"Cargando datos de hosp...\")\n",
    "cargar_y_dividir_datos(ruta_hosp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesan los 4 subconjuntos: todos los archivos pertenecientes a cada subconjunto son concatenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: subconjunto1_prescriptionscsv\n",
      "Cargando: subconjunto1_poe_detailcsv\n",
      "Cargando: subconjunto1_poecsv\n",
      "Advertencia: subconjunto1_d_icd_procedurescsv no tiene 'subject_id'.\n",
      "Cargando: subconjunto1_omrcsv\n",
      "Dataset hosp unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Concatenacion subconjunto 1\n",
    "ruta_carpeta = \"datasets\"\n",
    "df_hosp = []\n",
    "\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.startswith(\"subconjunto1\"):\n",
    "        ruta_csv = os.path.join(ruta_carpeta, archivo)\n",
    "        df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "        if 'subject_id' in df.columns:\n",
    "            print(f\"Cargando: {archivo}\")\n",
    "            df_hosp.append(df)\n",
    "        else:\n",
    "            print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_hosp:\n",
    "    hosp_dataset = pd.concat(df_hosp, ignore_index=True)\n",
    "    hosp_dataset.to_csv(\"datasets/hosp_dataset_sub1.csv\", index=False)\n",
    "    print(\"Dataset hosp unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88762729, 34)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 1\n",
    "hosp_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advertencia: subconjunto2_d_icd_diagnosescsv no tiene 'subject_id'.\n",
      "Cargando: subconjunto2_pharmacycsv\n",
      "Cargando: subconjunto2_diagnoses_icdcsv\n",
      "Cargando: subconjunto2_servicescsv\n",
      "Cargando: subconjunto2_emar_detailcsv\n",
      "Dataset hosp unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Concatenacion subconjunto 2\n",
    "ruta_carpeta = \"datasets\"\n",
    "df_hosp = []\n",
    "\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.startswith(\"subconjunto2\"):\n",
    "        ruta_csv = os.path.join(ruta_carpeta, archivo)\n",
    "        df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "        if 'subject_id' in df.columns:\n",
    "            print(f\"Cargando: {archivo}\")\n",
    "            df_hosp.append(df)\n",
    "        else:\n",
    "            print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_hosp:\n",
    "    hosp_dataset = pd.concat(df_hosp, ignore_index=True)\n",
    "    hosp_dataset.to_csv(\"datasets/hosp_dataset_sub2.csv\", index=False)\n",
    "    print(\"Dataset hosp unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112176190, 63)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 2\n",
    "hosp_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: subconjunto3_drgcodescsv\n",
      "Cargando: subconjunto3_labeventscsv\n",
      "Advertencia: subconjunto3_d_labitemscsv no tiene 'subject_id'.\n",
      "Cargando: subconjunto3_emarcsv\n",
      "Cargando: subconjunto3_microbiologyeventscsv\n",
      "Dataset hosp unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Concatenacion subconjunto 3\n",
    "ruta_carpeta = \"datasets\"\n",
    "df_hosp = []\n",
    "\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.startswith(\"subconjunto3\"):\n",
    "        ruta_csv = os.path.join(ruta_carpeta, archivo)\n",
    "        df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "        if 'subject_id' in df.columns:\n",
    "            print(f\"Cargando: {archivo}\")\n",
    "            df_hosp.append(df)\n",
    "        else:\n",
    "            print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_hosp:\n",
    "    hosp_dataset = pd.concat(df_hosp, ignore_index=True)\n",
    "    hosp_dataset.to_csv(\"datasets/hosp_dataset_sub3.csv\", index=False)\n",
    "    print(\"Dataset hosp unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205933437, 48)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 3\n",
    "hosp_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: subconjunto4_hcpcseventscsv\n",
      "Advertencia: subconjunto4_providercsv no tiene 'subject_id'.\n",
      "Cargando: subconjunto4_admissionscsv\n",
      "Cargando: subconjunto4_patientscsv\n",
      "Cargando: subconjunto4_transferscsv\n",
      "Advertencia: subconjunto4_d_hcpcscsv no tiene 'subject_id'.\n",
      "Cargando: subconjunto4_procedures_icdcsv\n",
      "Dataset hosp unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Concatenacion subconjunto 4\n",
    "ruta_carpeta = \"datasets\"\n",
    "df_hosp = []\n",
    "\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.startswith(\"subconjunto4\"):\n",
    "        ruta_csv = os.path.join(ruta_carpeta, archivo)\n",
    "        df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "        if 'subject_id' in df.columns:\n",
    "            print(f\"Cargando: {archivo}\")\n",
    "            df_hosp.append(df)\n",
    "        else:\n",
    "            print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_hosp:\n",
    "    hosp_dataset = pd.concat(df_hosp, ignore_index=True)\n",
    "    hosp_dataset.to_csv(\"datasets/hosp_dataset_sub4.csv\", index=False)\n",
    "    print(\"Dataset hosp unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4369965, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 4\n",
    "hosp_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesar datos icu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, los archivos .csv comprimidos en la carpeta icu son descomprimidos y asignados a 4 subconjuntos, aunque más tarde esta asignación no será utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de icu...\n",
      "Procesando: procedureevents (subconjunto1)\n",
      "Guardado: subconjunto1_procedureevents.csv (808706 filas)\n",
      "Procesando: ingredientevents (subconjunto1)\n",
      "Guardado: subconjunto1_ingredientevents.csv (14253480 filas)\n",
      "Procesando: d_items (subconjunto2)\n",
      "Guardado: subconjunto2_d_items.csv (4095 filas)\n",
      "Procesando: icustays (subconjunto2)\n",
      "Guardado: subconjunto2_icustays.csv (94458 filas)\n",
      "Procesando: datetimeevents (subconjunto3)\n",
      "Guardado: subconjunto3_datetimeevents.csv (9979761 filas)\n",
      "Procesando: chartevents (subconjunto3)\n",
      "Guardado: subconjunto3_chartevents_part0.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part1.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part2.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part3.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part4.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part5.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part6.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part7.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part8.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part9.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part10.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part11.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part12.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part13.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part14.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part15.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part16.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part17.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part18.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part19.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part20.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part21.csv (12997491 filas)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_principal = r\"/home/raulmartinez/medicalReports/mimic-iv-3.1\"\n",
    "\n",
    "# Función para leer archivos y dividirlo en 4 subconjuntos\n",
    "def cargar_y_dividir_datos(ruta_modulo):\n",
    "    archivos = [archivo for archivo in os.listdir(ruta_modulo) if archivo.endswith(\".gz\")]\n",
    "    total_archivos = len(archivos)\n",
    "    tam_subconjunto = total_archivos // 4\n",
    "\n",
    "    for i, archivo in enumerate(archivos):\n",
    "        ruta_gz = os.path.join(ruta_modulo, archivo)\n",
    "        nombre_tabla = archivo.replace(\".csv.gz\", \"\")  # Nombre de la tabla sin la extensión\n",
    "\n",
    "        if i < tam_subconjunto:\n",
    "            parte = \"subconjunto1\"\n",
    "        elif i < 2*tam_subconjunto:\n",
    "            parte = \"subconjunto2\"\n",
    "        elif i < 3*tam_subconjunto:\n",
    "            parte = \"subconjunto3\"\n",
    "        else:\n",
    "            parte = \"subconjunto4\"\n",
    "        \n",
    "        print(f\"Procesando: {nombre_tabla} ({parte})\")\n",
    "\n",
    "        # Leer chartevents en chunks (3GB en total)\n",
    "        if archivo.startswith(\"chartevents\"):\n",
    "            # Leer en chunks y guardar en subconjuntos por problemas de RAM\n",
    "            for j, chunk in enumerate(pd.read_csv(ruta_gz, compression=\"gzip\", chunksize=20000000, low_memory=False)):\n",
    "                chunk.to_csv(f\"datasets/icu/{parte}_{nombre_tabla}_part{j}.csv\", index=False)\n",
    "                print(f\"Guardado: {parte}_{nombre_tabla}_part{j}.csv ({chunk.shape[0]} filas)\")\n",
    "        \n",
    "        df = pd.read_csv(ruta_gz, compression=\"gzip\", low_memory=False)\n",
    "        df.to_csv(f\"datasets/icu/{parte}_{nombre_tabla}.csv\", index=False)\n",
    "        print(f\"Guardado: {parte}_{nombre_tabla}.csv ({df.shape[0]} filas)\")\n",
    "\n",
    "# Cargar datos de icu\n",
    "ruta_icu = os.path.join(ruta_principal, \"icu\")\n",
    "print (\"Cargando datos de icu...\")\n",
    "cargar_y_dividir_datos(ruta_icu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aqui descomprimidos subconjunto1, 2 y 3. La siguiente celda preprocesa solo el subconjunto4, que por problemas de RAM se subdivide en más archivos que posteriormente seran concatenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de icu...\n",
      "Procesando: procedureevents (subconjunto1)\n",
      "Procesando: ingredientevents (subconjunto1)\n",
      "Procesando: d_items (subconjunto2)\n",
      "Procesando: icustays (subconjunto2)\n",
      "Procesando: datetimeevents (subconjunto3)\n",
      "Procesando: chartevents (subconjunto3)\n",
      "Procesando: inputevents (subconjunto4)\n",
      "Guardado: subconjunto4_inputevents.csv (10953713 filas)\n",
      "Procesando: caregiver (subconjunto4)\n",
      "Guardado: subconjunto4_caregiver.csv (17984 filas)\n",
      "Procesando: outputevents (subconjunto4)\n",
      "Guardado: subconjunto4_outputevents.csv (5359395 filas)\n"
     ]
    }
   ],
   "source": [
    "# CARGAR DATOS EN SUBCONJUNTOS\n",
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_principal = r\"/home/raulmartinez/medicalReports/mimic-iv-3.1\"\n",
    "\n",
    "# Función para leer archivos y dividirlo en dos subconjuntos\n",
    "def cargar_y_dividir_datos(ruta_modulo):\n",
    "    archivos = [archivo for archivo in os.listdir(ruta_modulo) if archivo.endswith(\".gz\")]\n",
    "    total_archivos = len(archivos)\n",
    "    tam_subconjunto = total_archivos // 4\n",
    "\n",
    "    for i, archivo in enumerate(archivos):\n",
    "        ruta_gz = os.path.join(ruta_modulo, archivo)\n",
    "        nombre_tabla = archivo.replace(\".csv.gz\", \"\")  # Nombre de la tabla sin la extensión\n",
    "\n",
    "        if i < tam_subconjunto:\n",
    "            parte = \"subconjunto1\"\n",
    "        elif i < 2*tam_subconjunto:\n",
    "            parte = \"subconjunto2\"\n",
    "        elif i < 3*tam_subconjunto:\n",
    "            parte = \"subconjunto3\"\n",
    "        else:\n",
    "            parte = \"subconjunto4\"\n",
    "        \n",
    "        print(f\"Procesando: {nombre_tabla} ({parte})\")\n",
    "\n",
    "        if parte == \"subconjunto4\":\n",
    "            df = pd.read_csv(ruta_gz, compression=\"gzip\", low_memory=False)\n",
    "            df.to_csv(f\"datasets/icu/{parte}_{nombre_tabla}.csv\", index=False)\n",
    "            print(f\"Guardado: {parte}_{nombre_tabla}.csv ({df.shape[0]} filas)\")\n",
    "\n",
    "# Cargar datos de hosp\n",
    "ruta_icu = os.path.join(ruta_principal, \"icu\")\n",
    "print (\"Cargando datos de icu...\")\n",
    "cargar_y_dividir_datos(ruta_icu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para concatenar los 4 subconjuntos de la carpeta icu, se crea la función que guarda las matrices de los nombres de los archivos de cada subconjunto. Esta si es la asignación de los subconjuntos que sí se va a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevos subconjuntos de icu\n",
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_datasets = r\"/home/raulmartinez/medicalReports/datasets/icu\"\n",
    "\n",
    "matriz_nombres_archivo_sub1 = []\n",
    "matriz_nombres_archivo_sub2 = []\n",
    "matriz_nombres_archivo_sub3 = []\n",
    "matriz_nombres_archivo_sub4 = []\n",
    "\n",
    "# Función para guardar las matrices de los nombres de los archivos de los 4 subconjuntos  de icu\n",
    "def calcular_nuevo_subconjunto(ruta_modulo):\n",
    "    archivos = [archivo for archivo in os.listdir(ruta_modulo) if archivo.endswith(\".csv\")]\n",
    "    total_archivos = len(archivos)\n",
    "    tam_subconjunto = total_archivos // 4\n",
    "\n",
    "    for i, archivo in enumerate(archivos):\n",
    "        ruta_gz = os.path.join(ruta_modulo, archivo)\n",
    "\n",
    "        if i < tam_subconjunto:\n",
    "            matriz_nombres_archivo_sub1.append(ruta_gz)\n",
    "        elif i < 2*tam_subconjunto:\n",
    "            matriz_nombres_archivo_sub2.append(ruta_gz)\n",
    "        elif i < 3*tam_subconjunto:\n",
    "            matriz_nombres_archivo_sub3.append(ruta_gz)\n",
    "        else:\n",
    "            matriz_nombres_archivo_sub4.append(ruta_gz)\n",
    "\n",
    "# Cargar datos de icu\n",
    "calcular_nuevo_subconjunto(ruta_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a unificar los 4 subconjuntos de icu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part21.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part3.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto4_outputevents.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part8.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part4.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part5.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part15.csv\n",
      "Dataset icu unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Unificando subconjunto 1 de icu\n",
    "df_icu = []\n",
    "\n",
    "for archivo in matriz_nombres_archivo_sub1:\n",
    "    df = pd.read_csv(archivo, low_memory=False)\n",
    "    if 'subject_id' in df.columns:\n",
    "        print(f\"Cargando: {archivo}\")\n",
    "        df_icu.append(df)\n",
    "    else:\n",
    "        print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_icu:\n",
    "    icu_dataset = pd.concat(df_icu, ignore_index=True)\n",
    "    icu_dataset.to_csv(\"datasets/icu_dataset_sub1.csv\", index=False)\n",
    "    print(\"Dataset icu unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part13.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto1_procedureevents.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto4_inputevents.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part20.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part16.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part19.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part18.csv\n",
      "Dataset icu unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Unificando subconjunto 2 de icu\n",
    "df_icu = []\n",
    "\n",
    "for archivo in matriz_nombres_archivo_sub2:\n",
    "    df = pd.read_csv(archivo, low_memory=False)\n",
    "    if 'subject_id' in df.columns:\n",
    "        print(f\"Cargando: {archivo}\")\n",
    "        df_icu.append(df)\n",
    "    else:\n",
    "        print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_icu:\n",
    "    icu_dataset = pd.concat(df_icu, ignore_index=True)\n",
    "    icu_dataset.to_csv(\"datasets/icu_dataset_sub2.csv\", index=False)\n",
    "    print(\"Dataset icu unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111762419, 33)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 2 de icu\n",
    "icu_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto1_ingredientevents.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part1.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part17.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part10.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto2_icustays.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part6.csv\n",
      "Advertencia: /home/raulmartinez/medicalReports/datasets/icu/subconjunto4_caregiver.csv no tiene 'subject_id'.\n",
      "Dataset icu unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Unificando subconjunto 3 de icu\n",
    "df_icu = []\n",
    "\n",
    "for archivo in matriz_nombres_archivo_sub3:\n",
    "    df = pd.read_csv(archivo, low_memory=False)\n",
    "    if 'subject_id' in df.columns:\n",
    "        print(f\"Cargando: {archivo}\")\n",
    "        df_icu.append(df)\n",
    "    else:\n",
    "        print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_icu:\n",
    "    icu_dataset = pd.concat(df_icu, ignore_index=True)\n",
    "    icu_dataset.to_csv(\"datasets/icu_dataset_sub3.csv\", index=False)\n",
    "    print(\"Dataset icu unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94347938, 27)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 3 de icu\n",
    "icu_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part7.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part2.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part0.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part14.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part12.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part11.csv\n",
      "Advertencia: /home/raulmartinez/medicalReports/datasets/icu/subconjunto2_d_items.csv no tiene 'subject_id'.\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_datetimeevents.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part9.csv\n",
      "Dataset icu unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Unificando subconjunto 4 de icu\n",
    "df_icu = []\n",
    "\n",
    "for archivo in matriz_nombres_archivo_sub4:\n",
    "    df = pd.read_csv(archivo, low_memory=False)\n",
    "    if 'subject_id' in df.columns:\n",
    "        print(f\"Cargando: {archivo}\")\n",
    "        df_icu.append(df)\n",
    "    else:\n",
    "        print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_icu:\n",
    "    icu_dataset = pd.concat(df_icu, ignore_index=True)\n",
    "    icu_dataset.to_csv(\"datasets/icu_dataset_sub4.csv\", index=False)\n",
    "    print(\"Dataset icu unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149979761, 11)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 4 de icu\n",
    "icu_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtar datasets hosp e icu con subjects_id presentes en notes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset de las notas (notes) ha sido preprocesado en el archivo ``preprocessing_notes.ipynb`` . Ahí se ha ejecutado un código para guardar un fichero de texto que contiene todos los valores subject_id que contiene ese dataset. En esta parte lo que se hace es eliminar las filas de los subconjuntos de hosp e icu que contienen valores subject_id que no están presentes en el dataset notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_principal = r\"/home/raulmartinez/medicalReports/datasets\"\n",
    "\n",
    "# Función para leer archivos y filtarlos para conservar solo los subject_id presentes en el texto\n",
    "def cargar_datos_subject_id(ruta_subconjunto, ruta_archivo_subject_id):\n",
    "    # Cargar csv\n",
    "    df = pd.read_csv(ruta_subconjunto, low_memory=False)\n",
    "    print(f\"Leido: {ruta_subconjunto} ({df.shape[0]} filas)\")\n",
    "\n",
    "    # Leer los subject_id desde el archivo de texto\n",
    "    with open(ruta_archivo_subject_id, \"r\") as f:\n",
    "        subject_ids = set(line.strip() for line in f)\n",
    "\n",
    "    # Filtrar el DataFrame para conservar solo los subject_id que están en el archivo de texto\n",
    "    df_filtrado = df[df['subject_id'].astype(str).isin(subject_ids)]\n",
    "\n",
    "    # Guardar el nuevo conjunto de datos\n",
    "    carpeta, archivo = os.path.split(ruta_subconjunto)\n",
    "    df_filtrado.to_csv(f\"datasets/union_hosp_icu_notes/{archivo}\", index=False)\n",
    "    print(f\"Guardado: datasets/union_hosp_icu_notes/{archivo}.csv ({df_filtrado.shape[0]} filas)\")\n",
    "    \n",
    "\n",
    "    # Mostrar los primeros resultados\n",
    "    print(df_filtrado.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por problemas de espacio se crea la función siguiente, que es igual que la anterior, solo que lee el subconjunto en varios chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer archivos y filtarlos para conservar solo los subject_id presentes en el texto\n",
    "def cargar_datos_subject_id_chunks(ruta_subconjunto, ruta_archivo_subject_id):\n",
    "\n",
    "    # Leer los subject_id desde el archivo de texto\n",
    "    with open(ruta_archivo_subject_id, \"r\") as f:\n",
    "        subject_ids = set(line.strip() for line in f)\n",
    "    \n",
    "    # Obtener el número total de filas para dividir en 8 chunks\n",
    "    total_rows = sum(1 for _ in open(ruta_subconjunto)) - 1  # Restar 1 por la cabecera\n",
    "    chunk_size = total_rows // 8  # Dividir en 8 partes\n",
    "\n",
    "    print(f\"Procesando: {ruta_subconjunto} ({total_rows} filas)\")\n",
    "    \n",
    "    carpeta, archivo = os.path.split(ruta_subconjunto)\n",
    "\n",
    "    # Leer el CSV en 8 chunks y filtrar cada uno\n",
    "    primer_chunk = True\n",
    "    for chunk in pd.read_csv(ruta_subconjunto, chunksize=chunk_size, low_memory=False):\n",
    "        chunk_filtrado = chunk[chunk['subject_id'].astype(str).isin(subject_ids)]\n",
    "        \n",
    "        # Guardar el nuevo conjunto de datos\n",
    "        chunk_filtrado.to_csv(f\"datasets/union_hosp_icu_notes/{archivo}\",mode='w' if primer_chunk else 'a', index=False, header=primer_chunk)\n",
    "        primer_chunk = False\n",
    "\n",
    "    \n",
    "    new_total_rows = sum(1 for _ in open(f\"datasets/union_hosp_icu_notes/{archivo}\")) - 1\n",
    "    print(f\"Guardado: datasets/union_hosp_icu_notes/{archivo} ({new_total_rows} filas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Leido: /home/raulmartinez/medicalReports/datasets/hosp/hosp_dataset_sub1.csv (88762729 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub1.csv.csv (74990340 filas)\n",
      "   subject_id     hadm_id  pharmacy_id       poe_id  poe_seq  \\\n",
      "0    10000032  22595853.0   12775705.0  10000032-55     55.0   \n",
      "1    10000032  22595853.0   18415984.0  10000032-42     42.0   \n",
      "2    10000032  22595853.0   23637373.0  10000032-35     35.0   \n",
      "3    10000032  22595853.0   26862314.0  10000032-41     41.0   \n",
      "4    10000032  22595853.0   30740602.0  10000032-27     27.0   \n",
      "\n",
      "  order_provider_id            starttime             stoptime drug_type  \\\n",
      "0            P85UQ1  2180-05-08 08:00:00  2180-05-07 22:00:00      MAIN   \n",
      "1            P23SJA  2180-05-07 02:00:00  2180-05-07 22:00:00      MAIN   \n",
      "2            P23SJA  2180-05-07 01:00:00  2180-05-07 09:00:00      MAIN   \n",
      "3            P23SJA  2180-05-07 01:00:00  2180-05-07 01:00:00      MAIN   \n",
      "4            P23SJA  2180-05-07 00:00:00  2180-05-07 22:00:00      MAIN   \n",
      "\n",
      "                          drug  ... order_type order_subtype  \\\n",
      "0                   Furosemide  ...        NaN           NaN   \n",
      "1      Ipratropium Bromide Neb  ...        NaN           NaN   \n",
      "2                   Furosemide  ...        NaN           NaN   \n",
      "3           Potassium Chloride  ...        NaN           NaN   \n",
      "4  Sodium Chloride 0.9%  Flush  ...        NaN           NaN   \n",
      "\n",
      "   transaction_type discontinue_of_poe_id discontinued_by_poe_id order_status  \\\n",
      "0               NaN                   NaN                    NaN          NaN   \n",
      "1               NaN                   NaN                    NaN          NaN   \n",
      "2               NaN                   NaN                    NaN          NaN   \n",
      "3               NaN                   NaN                    NaN          NaN   \n",
      "4               NaN                   NaN                    NaN          NaN   \n",
      "\n",
      "  chartdate seq_num result_name  result_value  \n",
      "0       NaN     NaN         NaN           NaN  \n",
      "1       NaN     NaN         NaN           NaN  \n",
      "2       NaN     NaN         NaN           NaN  \n",
      "3       NaN     NaN         NaN           NaN  \n",
      "4       NaN     NaN         NaN           NaN  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 1 de hosp\n",
    "ruta_csv = os.path.join(ruta_principal, \"hosp/hosp_dataset_sub1.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/hosp/hosp_dataset_sub2.csv (112176190 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv.csv (112176190 filas)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 2 de hosp\n",
    "ruta_csv = os.path.join(ruta_principal, \"hosp/hosp_dataset_sub2.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id_chunks(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv (89822760 filas)\n"
     ]
    }
   ],
   "source": [
    "# Total filas nuevo subconjunto 2\n",
    "new_total_rows = sum(1 for _ in open(f\"datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv\")) - 1\n",
    "print(f\"Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv ({new_total_rows} filas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/hosp/hosp_dataset_sub3.csv (205933437 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv (171703463 filas)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 3 de hosp\n",
    "ruta_csv = os.path.join(ruta_principal, \"hosp/hosp_dataset_sub3.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id_chunks(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Leido: /home/raulmartinez/medicalReports/datasets/hosp/hosp_dataset_sub4.csv (4369965 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub4.csv.csv (3634667 filas)\n",
      "   subject_id     hadm_id   chartdate hcpcs_cd  seq_num  \\\n",
      "0    10000068  25022803.0  2160-03-04    99218      1.0   \n",
      "1    10000084  29888819.0  2160-12-28    G0378      1.0   \n",
      "2    10000108  27250926.0  2163-09-27    99219      1.0   \n",
      "3    10000117  22927623.0  2181-11-15    43239      1.0   \n",
      "4    10000117  22927623.0  2181-11-15    G0378      2.0   \n",
      "\n",
      "               short_description admittime dischtime deathtime admission_type  \\\n",
      "0  Hospital observation services       NaN       NaN       NaN            NaN   \n",
      "1    Hospital observation per hr       NaN       NaN       NaN            NaN   \n",
      "2  Hospital observation services       NaN       NaN       NaN            NaN   \n",
      "3               Digestive system       NaN       NaN       NaN            NaN   \n",
      "4    Hospital observation per hr       NaN       NaN       NaN            NaN   \n",
      "\n",
      "   ... anchor_year anchor_year_group  dod transfer_id eventtype careunit  \\\n",
      "0  ...         NaN               NaN  NaN         NaN       NaN      NaN   \n",
      "1  ...         NaN               NaN  NaN         NaN       NaN      NaN   \n",
      "2  ...         NaN               NaN  NaN         NaN       NaN      NaN   \n",
      "3  ...         NaN               NaN  NaN         NaN       NaN      NaN   \n",
      "4  ...         NaN               NaN  NaN         NaN       NaN      NaN   \n",
      "\n",
      "  intime outtime icd_code  icd_version  \n",
      "0    NaN     NaN      NaN          NaN  \n",
      "1    NaN     NaN      NaN          NaN  \n",
      "2    NaN     NaN      NaN          NaN  \n",
      "3    NaN     NaN      NaN          NaN  \n",
      "4    NaN     NaN      NaN          NaN  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 4 de hosp\n",
    "ruta_csv = os.path.join(ruta_principal, \"hosp/hosp_dataset_sub4.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Leido: /home/raulmartinez/medicalReports/datasets/icu/icu_dataset_sub1.csv (118356886 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/icu_dataset_sub1.csv.csv (94840246 filas)\n",
      "   subject_id   hadm_id   stay_id  caregiver_id            charttime  \\\n",
      "0    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "1    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "2    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "3    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "4    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "\n",
      "             storetime  itemid value  valuenum  valueuom  warning  \n",
      "0  2140-09-25 11:14:00  220180    86      86.0      mmHg      0.0  \n",
      "1  2140-09-25 11:14:00  220181    93      93.0      mmHg      0.0  \n",
      "2  2140-09-25 11:14:00  220210    24      24.0  insp/min      0.0  \n",
      "3  2140-09-25 11:14:00  220277    99      99.0         %      0.0  \n",
      "4  2140-09-25 11:15:00  224650   NaN       NaN       NaN      0.0  \n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 1 de icu\n",
    "ruta_csv = os.path.join(ruta_principal, \"icu/icu_dataset_sub1.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/icu/icu_dataset_sub2.csv (111762419 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/icu_dataset_sub2.csv (89948910 filas)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 2 de icu\n",
    "ruta_csv = os.path.join(ruta_principal, \"icu/icu_dataset_sub2.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id_chunks(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/icu/icu_dataset_sub3.csv (94347938 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/icu_dataset_sub3.csv (77276054 filas)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 3 de icu\n",
    "ruta_csv = os.path.join(ruta_principal, \"icu/icu_dataset_sub3.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id_chunks(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/icu/icu_dataset_sub4.csv (149979761 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/icu_dataset_sub4.csv (124732175 filas)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 4 de icu\n",
    "ruta_csv = os.path.join(ruta_principal, \"icu/icu_dataset_sub4.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id_chunks(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenación de todos los subconjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas de icu_dataset_sub3.csv:\n",
      "   subject_id   hadm_id   stay_id  caregiver_id            starttime  \\\n",
      "0    10000032  29079034  39553978       18704.0  2180-07-23 17:00:00   \n",
      "1    10000032  29079034  39553978       18704.0  2180-07-23 17:00:00   \n",
      "2    10000032  29079034  39553978       18704.0  2180-07-23 17:00:00   \n",
      "3    10000032  29079034  39553978       18704.0  2180-07-23 17:00:00   \n",
      "4    10000032  29079034  39553978       18704.0  2180-07-23 17:33:00   \n",
      "\n",
      "               endtime            storetime    itemid      amount amountuom  \\\n",
      "0  2180-07-23 17:01:00  2180-07-23 18:56:00  220490.0  200.000000        mL   \n",
      "1  2180-07-23 17:01:00  2180-07-23 18:56:00  227075.0  200.000000        mL   \n",
      "2  2180-07-23 17:30:00  2180-07-23 17:02:00  220490.0   49.999999        mL   \n",
      "3  2180-07-23 17:30:00  2180-07-23 17:02:00  226509.0   49.999999        mL   \n",
      "4  2180-07-23 18:03:00  2180-07-23 18:16:00  220490.0   49.999999        mL   \n",
      "\n",
      "   ...  charttime value  valuenum  valueuom warning  first_careunit  \\\n",
      "0  ...        NaN   NaN       NaN       NaN     NaN             NaN   \n",
      "1  ...        NaN   NaN       NaN       NaN     NaN             NaN   \n",
      "2  ...        NaN   NaN       NaN       NaN     NaN             NaN   \n",
      "3  ...        NaN   NaN       NaN       NaN     NaN             NaN   \n",
      "4  ...        NaN   NaN       NaN       NaN     NaN             NaN   \n",
      "\n",
      "   last_careunit  intime  outtime  los  \n",
      "0            NaN     NaN      NaN  NaN  \n",
      "1            NaN     NaN      NaN  NaN  \n",
      "2            NaN     NaN      NaN  NaN  \n",
      "3            NaN     NaN      NaN  NaN  \n",
      "4            NaN     NaN      NaN  NaN  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "Primeras filas de hosp_dataset_sub4.csv:\n",
      "   subject_id     hadm_id   chartdate hcpcs_cd  seq_num  \\\n",
      "0    10000068  25022803.0  2160-03-04    99218      1.0   \n",
      "1    10000084  29888819.0  2160-12-28    G0378      1.0   \n",
      "2    10000108  27250926.0  2163-09-27    99219      1.0   \n",
      "3    10000117  22927623.0  2181-11-15    43239      1.0   \n",
      "4    10000117  22927623.0  2181-11-15    G0378      2.0   \n",
      "\n",
      "               short_description  admittime  dischtime  deathtime  \\\n",
      "0  Hospital observation services        NaN        NaN        NaN   \n",
      "1    Hospital observation per hr        NaN        NaN        NaN   \n",
      "2  Hospital observation services        NaN        NaN        NaN   \n",
      "3               Digestive system        NaN        NaN        NaN   \n",
      "4    Hospital observation per hr        NaN        NaN        NaN   \n",
      "\n",
      "   admission_type  ...  anchor_year  anchor_year_group  dod  transfer_id  \\\n",
      "0             NaN  ...          NaN                NaN  NaN          NaN   \n",
      "1             NaN  ...          NaN                NaN  NaN          NaN   \n",
      "2             NaN  ...          NaN                NaN  NaN          NaN   \n",
      "3             NaN  ...          NaN                NaN  NaN          NaN   \n",
      "4             NaN  ...          NaN                NaN  NaN          NaN   \n",
      "\n",
      "   eventtype  careunit  intime  outtime  icd_code  icd_version  \n",
      "0        NaN       NaN     NaN      NaN       NaN          NaN  \n",
      "1        NaN       NaN     NaN      NaN       NaN          NaN  \n",
      "2        NaN       NaN     NaN      NaN       NaN          NaN  \n",
      "3        NaN       NaN     NaN      NaN       NaN          NaN  \n",
      "4        NaN       NaN     NaN      NaN       NaN          NaN  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "Primeras filas de icu_dataset_sub4.csv:\n",
      "   subject_id   hadm_id   stay_id  caregiver_id            charttime  \\\n",
      "0    13242029  26906343  32900001        8280.0  2126-05-11 12:00:00   \n",
      "1    13242029  26906343  32900001        8280.0  2126-05-11 12:00:00   \n",
      "2    13242029  26906343  32900001        8280.0  2126-05-11 12:00:00   \n",
      "3    13242029  26906343  32900001        8280.0  2126-05-11 12:00:00   \n",
      "4    13242029  26906343  32900001        8280.0  2126-05-11 12:00:00   \n",
      "\n",
      "             storetime  itemid            value  valuenum  valueuom  warning  \n",
      "0  2126-05-11 12:12:00  223947  Strong/Palpable       NaN       NaN      0.0  \n",
      "1  2126-05-11 12:12:00  223948  Strong/Palpable       NaN       NaN      0.0  \n",
      "2  2126-05-11 12:12:00  223976           Normal       NaN       NaN      0.0  \n",
      "3  2126-05-11 12:12:00  223979           Normal       NaN       NaN      0.0  \n",
      "4  2126-05-11 12:12:00  223982           Normal       NaN       NaN      0.0  \n",
      "Primeras filas de hosp_dataset_sub3.csv:\n",
      "   subject_id     hadm_id drg_type  drg_code  \\\n",
      "0    10000032  22595853.0      APR     283.0   \n",
      "1    10000032  22595853.0     HCFA     442.0   \n",
      "2    10000032  22841357.0      APR     279.0   \n",
      "3    10000032  22841357.0     HCFA     442.0   \n",
      "4    10000032  25742920.0      APR     283.0   \n",
      "\n",
      "                                         description  drg_severity  \\\n",
      "0                       OTHER DISORDERS OF THE LIVER           2.0   \n",
      "1  DISORDERS OF LIVER EXCEPT MALIG,CIRR,ALC HEPA ...           NaN   \n",
      "2  HEPATIC COMA AND OTHER MAJOR ACUTE LIVER DISOR...           3.0   \n",
      "3  DISORDERS OF LIVER EXCEPT MALIG,CIRR,ALC HEPA ...           NaN   \n",
      "4                       OTHER DISORDERS OF THE LIVER           3.0   \n",
      "\n",
      "   drg_mortality  labevent_id  specimen_id  itemid  ...  org_itemid  org_name  \\\n",
      "0            2.0          NaN          NaN     NaN  ...         NaN       NaN   \n",
      "1            NaN          NaN          NaN     NaN  ...         NaN       NaN   \n",
      "2            2.0          NaN          NaN     NaN  ...         NaN       NaN   \n",
      "3            NaN          NaN          NaN     NaN  ...         NaN       NaN   \n",
      "4            2.0          NaN          NaN     NaN  ...         NaN       NaN   \n",
      "\n",
      "   isolate_num  quantity  ab_itemid  ab_name  dilution_text  \\\n",
      "0          NaN       NaN        NaN      NaN            NaN   \n",
      "1          NaN       NaN        NaN      NaN            NaN   \n",
      "2          NaN       NaN        NaN      NaN            NaN   \n",
      "3          NaN       NaN        NaN      NaN            NaN   \n",
      "4          NaN       NaN        NaN      NaN            NaN   \n",
      "\n",
      "   dilution_comparison  dilution_value  interpretation  \n",
      "0                  NaN             NaN             NaN  \n",
      "1                  NaN             NaN             NaN  \n",
      "2                  NaN             NaN             NaN  \n",
      "3                  NaN             NaN             NaN  \n",
      "4                  NaN             NaN             NaN  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "Primeras filas de icu_dataset_sub1.csv:\n",
      "   subject_id   hadm_id   stay_id  caregiver_id            charttime  \\\n",
      "0    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "1    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "2    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "3    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "4    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "\n",
      "             storetime  itemid  value  valuenum  valueuom  warning  \n",
      "0  2140-09-25 11:14:00  220180   86.0      86.0      mmHg      0.0  \n",
      "1  2140-09-25 11:14:00  220181   93.0      93.0      mmHg      0.0  \n",
      "2  2140-09-25 11:14:00  220210   24.0      24.0  insp/min      0.0  \n",
      "3  2140-09-25 11:14:00  220277   99.0      99.0         %      0.0  \n",
      "4  2140-09-25 11:15:00  224650    NaN       NaN       NaN      0.0  \n",
      "Primeras filas de hosp_dataset_sub2.csv:\n",
      "   subject_id     hadm_id  pharmacy_id       poe_id            starttime  \\\n",
      "0    10000032  22595853.0   12775705.0  10000032-55  2180-05-08 08:00:00   \n",
      "1    10000032  22595853.0   18415984.0  10000032-42  2180-05-07 02:00:00   \n",
      "2    10000032  22595853.0   23637373.0  10000032-35  2180-05-07 01:00:00   \n",
      "3    10000032  22595853.0   26862314.0  10000032-41  2180-05-07 01:00:00   \n",
      "4    10000032  22595853.0   30740602.0  10000032-27  2180-05-07 00:00:00   \n",
      "\n",
      "              stoptime                   medication  proc_type  \\\n",
      "0  2180-05-07 22:00:00                   Furosemide  Unit Dose   \n",
      "1  2180-05-07 22:00:00      Ipratropium Bromide Neb  Unit Dose   \n",
      "2  2180-05-07 09:00:00                   Furosemide  Unit Dose   \n",
      "3  2180-05-07 01:00:00           Potassium Chloride  Unit Dose   \n",
      "4  2180-05-07 22:00:00  Sodium Chloride 0.9%  Flush  Unit Dose   \n",
      "\n",
      "                               status            entertime  ...  \\\n",
      "0  Discontinued via patient discharge  2180-05-07 09:32:35  ...   \n",
      "1  Discontinued via patient discharge  2180-05-07 01:49:23  ...   \n",
      "2    Inactive (Due to a change order)  2180-05-07 00:09:24  ...   \n",
      "3                        Discontinued  2180-05-07 00:09:24  ...   \n",
      "4  Discontinued via patient discharge  2180-05-07 00:00:54  ...   \n",
      "\n",
      "  infusion_rate_adjustment_amount infusion_rate_unit infusion_complete  \\\n",
      "0                             NaN                NaN               NaN   \n",
      "1                             NaN                NaN               NaN   \n",
      "2                             NaN                NaN               NaN   \n",
      "3                             NaN                NaN               NaN   \n",
      "4                             NaN                NaN               NaN   \n",
      "\n",
      "  completion_interval  new_iv_bag_hung  continued_infusion_in_other_location  \\\n",
      "0                 NaN              NaN                                   NaN   \n",
      "1                 NaN              NaN                                   NaN   \n",
      "2                 NaN              NaN                                   NaN   \n",
      "3                 NaN              NaN                                   NaN   \n",
      "4                 NaN              NaN                                   NaN   \n",
      "\n",
      "   restart_interval  side  site  non_formulary_visual_verification  \n",
      "0               NaN   NaN   NaN                                NaN  \n",
      "1               NaN   NaN   NaN                                NaN  \n",
      "2               NaN   NaN   NaN                                NaN  \n",
      "3               NaN   NaN   NaN                                NaN  \n",
      "4               NaN   NaN   NaN                                NaN  \n",
      "\n",
      "[5 rows x 63 columns]\n",
      "Primeras filas de hosp_dataset_sub1.csv:\n",
      "   subject_id     hadm_id  pharmacy_id       poe_id  poe_seq  \\\n",
      "0    10000032  22595853.0   12775705.0  10000032-55     55.0   \n",
      "1    10000032  22595853.0   18415984.0  10000032-42     42.0   \n",
      "2    10000032  22595853.0   23637373.0  10000032-35     35.0   \n",
      "3    10000032  22595853.0   26862314.0  10000032-41     41.0   \n",
      "4    10000032  22595853.0   30740602.0  10000032-27     27.0   \n",
      "\n",
      "  order_provider_id            starttime             stoptime drug_type  \\\n",
      "0            P85UQ1  2180-05-08 08:00:00  2180-05-07 22:00:00      MAIN   \n",
      "1            P23SJA  2180-05-07 02:00:00  2180-05-07 22:00:00      MAIN   \n",
      "2            P23SJA  2180-05-07 01:00:00  2180-05-07 09:00:00      MAIN   \n",
      "3            P23SJA  2180-05-07 01:00:00  2180-05-07 01:00:00      MAIN   \n",
      "4            P23SJA  2180-05-07 00:00:00  2180-05-07 22:00:00      MAIN   \n",
      "\n",
      "                          drug  ... order_type  order_subtype  \\\n",
      "0                   Furosemide  ...        NaN            NaN   \n",
      "1      Ipratropium Bromide Neb  ...        NaN            NaN   \n",
      "2                   Furosemide  ...        NaN            NaN   \n",
      "3           Potassium Chloride  ...        NaN            NaN   \n",
      "4  Sodium Chloride 0.9%  Flush  ...        NaN            NaN   \n",
      "\n",
      "   transaction_type discontinue_of_poe_id  discontinued_by_poe_id  \\\n",
      "0               NaN                   NaN                     NaN   \n",
      "1               NaN                   NaN                     NaN   \n",
      "2               NaN                   NaN                     NaN   \n",
      "3               NaN                   NaN                     NaN   \n",
      "4               NaN                   NaN                     NaN   \n",
      "\n",
      "   order_status chartdate  seq_num result_name  result_value  \n",
      "0           NaN       NaN      NaN         NaN           NaN  \n",
      "1           NaN       NaN      NaN         NaN           NaN  \n",
      "2           NaN       NaN      NaN         NaN           NaN  \n",
      "3           NaN       NaN      NaN         NaN           NaN  \n",
      "4           NaN       NaN      NaN         NaN           NaN  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "Primeras filas de icu_dataset_sub2.csv:\n",
      "   subject_id   hadm_id   stay_id  caregiver_id            charttime  \\\n",
      "0    16035306  20325580  31722244       28759.0  2182-01-06 08:00:00   \n",
      "1    16035306  20325580  31722244       28759.0  2182-01-06 08:00:00   \n",
      "2    16035306  20325580  31722244       28759.0  2182-01-06 08:00:00   \n",
      "3    16035306  20325580  31722244       28759.0  2182-01-06 08:00:00   \n",
      "4    16035306  20325580  31722244       28759.0  2182-01-06 08:00:00   \n",
      "\n",
      "             storetime  itemid            value  valuenum  valueuom  ...  \\\n",
      "0  2182-01-06 09:58:00  225018              NaN       NaN       NaN  ...   \n",
      "1  2182-01-06 09:58:00  225023      Transparent       NaN       NaN  ...   \n",
      "2  2182-01-06 09:58:00  225028   Dry and intact       NaN       NaN  ...   \n",
      "3  2182-01-06 09:58:00  225043  Not applicable        NaN       NaN  ...   \n",
      "4  2182-01-06 09:58:00  227472          Sternum       NaN       NaN  ...   \n",
      "\n",
      "   originalamount  originalrate  amount  amountuom  rate  rateuom  \\\n",
      "0             NaN           NaN     NaN        NaN   NaN      NaN   \n",
      "1             NaN           NaN     NaN        NaN   NaN      NaN   \n",
      "2             NaN           NaN     NaN        NaN   NaN      NaN   \n",
      "3             NaN           NaN     NaN        NaN   NaN      NaN   \n",
      "4             NaN           NaN     NaN        NaN   NaN      NaN   \n",
      "\n",
      "   secondaryordercategoryname  ordercomponenttypedescription  totalamount  \\\n",
      "0                         NaN                            NaN          NaN   \n",
      "1                         NaN                            NaN          NaN   \n",
      "2                         NaN                            NaN          NaN   \n",
      "3                         NaN                            NaN          NaN   \n",
      "4                         NaN                            NaN          NaN   \n",
      "\n",
      "   totalamountuom  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "Primeras filas de dataset_notes_unificado.csv:\n",
      "          note_id  subject_id     hadm_id note_type  note_seq  \\\n",
      "0  10000032-DS-21    10000032  22595853.0        DS      21.0   \n",
      "1  10000032-DS-22    10000032  22841357.0        DS      22.0   \n",
      "2  10000032-DS-23    10000032  29079034.0        DS      23.0   \n",
      "3  10000032-DS-24    10000032  25742920.0        DS      24.0   \n",
      "4  10000084-DS-17    10000084  23052089.0        DS      17.0   \n",
      "\n",
      "             charttime            storetime  \\\n",
      "0  2180-05-07 00:00:00  2180-05-09 15:26:00   \n",
      "1  2180-06-27 00:00:00  2180-07-01 10:15:00   \n",
      "2  2180-07-25 00:00:00  2180-07-25 21:42:00   \n",
      "3  2180-08-07 00:00:00  2180-08-10 05:43:00   \n",
      "4  2160-11-25 00:00:00  2160-11-25 15:09:00   \n",
      "\n",
      "                                                text  field_name  field_value  \\\n",
      "0   \\nName:  ___                     Unit No:   _...         NaN          NaN   \n",
      "1   \\nName:  ___                     Unit No:   _...         NaN          NaN   \n",
      "2   \\nName:  ___                     Unit No:   _...         NaN          NaN   \n",
      "3   \\nName:  ___                     Unit No:   _...         NaN          NaN   \n",
      "4   \\nName:  ___                    Unit No:   __...         NaN          NaN   \n",
      "\n",
      "   ...  category_3  category_4  category_5  section_name  category_1_name  \\\n",
      "0  ...         NaN         NaN         NaN           NaN              NaN   \n",
      "1  ...         NaN         NaN         NaN           NaN              NaN   \n",
      "2  ...         NaN         NaN         NaN           NaN              NaN   \n",
      "3  ...         NaN         NaN         NaN           NaN              NaN   \n",
      "4  ...         NaN         NaN         NaN           NaN              NaN   \n",
      "\n",
      "   category_2_name  category_3_name  category_4_name  chapter_name  \\\n",
      "0              NaN              NaN              NaN           NaN   \n",
      "1              NaN              NaN              NaN           NaN   \n",
      "2              NaN              NaN              NaN           NaN   \n",
      "3              NaN              NaN              NaN           NaN   \n",
      "4              NaN              NaN              NaN           NaN   \n",
      "\n",
      "   super_section_name  \n",
      "0                 NaN  \n",
      "1                 NaN  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "4                 NaN  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Guardado: datasets/union_hosp_icu_notes/final_dataset.csv (45 filas)\n"
     ]
    }
   ],
   "source": [
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_principal = r\"/home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes\"\n",
    "\n",
    "def cargar_primeras_filas(ruta_modulo, archivo_final):\n",
    "    df_archivos = []\n",
    "    archivos = [archivo for archivo in os.listdir(ruta_modulo) if archivo.endswith(\".csv\")]\n",
    "    for archivo in archivos:\n",
    "        ruta_csv = os.path.join(ruta_modulo, archivo)\n",
    "        # Leer solo las primeras 5 filas del archivo CSV\n",
    "        df = pd.read_csv(ruta_csv, low_memory=False, nrows=5)\n",
    "        # Mostrar las primeras filas del DataFrame\n",
    "        print(f\"Primeras filas de {archivo}:\")\n",
    "        print(df.head())\n",
    "        df_archivos.append(df)\n",
    "\n",
    "    # Concatenar todos los DataFrames en uno solo\n",
    "    df_final = pd.concat(df_archivos, ignore_index=True)\n",
    "    # Guardar el nuevo conjunto de datos\n",
    "    df_final.to_csv(archivo_final, index=False)\n",
    "    print(f\"Guardado: {archivo_final} ({df_final.shape[0]} filas)\")\n",
    "\n",
    "# Cargar las primeras filas de los archivos\n",
    "archivo_final = \"datasets/union_hosp_icu_notes/final_dataset.csv\"\n",
    "cargar_primeras_filas(ruta_principal, archivo_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer archivos y cargarlos en chunks para ser unidos en un solo archivo\n",
    "def concatenar_subconjuntos_chunks(ruta_subconjunto):\n",
    "\n",
    "    print(f\"Procesando: {ruta_subconjunto}\")\n",
    "\n",
    "    chunk_size = 20000000  # Tamaño del chunk (20 millones de filas)\n",
    "    \n",
    "    # Leer el CSV en varios chunks e ir añadiendolo al conjunto final\n",
    "    for chunk in pd.read_csv(ruta_subconjunto,\n",
    "                            chunksize=chunk_size,\n",
    "                            skiprows=5,\n",
    "                            header=None,\n",
    "                            low_memory=False):\n",
    "        # Guardar el nuevo conjunto de datos\n",
    "        chunk.to_csv(f\"datasets/union_hosp_icu_notes/final_dataset.csv\",mode='a', index=False, header=False)\n",
    "        print(f\"Añadidas a dataset_final {chunk.shape[0]} filas del archivo {ruta_subconjunto}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de <l primer subconjunto de hosp...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub1.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub1.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub1.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub1.csv\n",
      "Añadidas a dataset_final 14990336 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub1.csv\n"
     ]
    }
   ],
   "source": [
    "ruta_principal = r\"/home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes\"\n",
    "ruta_archivo = os.path.join(ruta_principal, \"hosp_dataset_sub1.csv\")\n",
    "print (\"Cargando datos de el primer subconjunto de hosp...\")\n",
    "concatenar_subconjuntos_chunks(ruta_archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos del segundo subconjunto de hosp...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv\n",
      "Añadidas a dataset_final 9822756 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv\n"
     ]
    }
   ],
   "source": [
    "ruta_principal = r\"/home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes\"\n",
    "ruta_archivo = os.path.join(ruta_principal, \"hosp_dataset_sub2.csv\")\n",
    "print (\"Cargando datos del segundo subconjunto de hosp...\")\n",
    "concatenar_subconjuntos_chunks(ruta_archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos del segundo subconjunto de hosp...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv\n",
      "Añadidas a dataset_final 11703459 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv\n"
     ]
    }
   ],
   "source": [
    "ruta_principal = r\"/home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes\"\n",
    "ruta_archivo = os.path.join(ruta_principal, \"hosp_dataset_sub3.csv\")\n",
    "print (\"Cargando datos del tercero subconjunto de hosp...\")\n",
    "concatenar_subconjuntos_chunks(ruta_archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos del segundo subconjunto de hosp...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub4.csv\n",
      "Añadidas a dataset_final 3634663 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/hosp_dataset_sub4.csv\n"
     ]
    }
   ],
   "source": [
    "ruta_principal = r\"/home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes\"\n",
    "ruta_archivo = os.path.join(ruta_principal, \"hosp_dataset_sub4.csv\")\n",
    "print (\"Cargando datos del cuarto subconjunto de hosp...\")\n",
    "concatenar_subconjuntos_chunks(ruta_archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos del primer subconjunto de icu...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/icu_dataset_sub1.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/icu_dataset_sub1.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/icu_dataset_sub1.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/icu_dataset_sub1.csv\n",
      "Añadidas a dataset_final 20000000 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/icu_dataset_sub1.csv\n",
      "Añadidas a dataset_final 14840242 filas del archivo /home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes/icu_dataset_sub1.csv\n"
     ]
    }
   ],
   "source": [
    "ruta_principal = r\"/home/raulmartinez/medicalReports/datasets/union_hosp_icu_notes\"\n",
    "ruta_archivo = os.path.join(ruta_principal, \"icu_dataset_sub1.csv\")\n",
    "print (\"Cargando datos del primer subconjunto de icu...\")\n",
    "concatenar_subconjuntos_chunks(ruta_archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiempo hosp: 23 + 43 + 82 + 1\n",
    "Tiempo icu: 12 + \n",
    "Tiempo notes: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
