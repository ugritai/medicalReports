{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesar datos hosp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, los archivos .csv comprimidos en la carpeta hosp son descomprimidos y asignados a 4 subconjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de hosp...\n",
      "Procesando: poe (subconjunto1)\n",
      "Guardado: subconjunto1_poe.csv (52212109 filas)\n",
      "Procesando: d_icd_procedures (subconjunto1)\n",
      "Guardado: subconjunto1_d_icd_procedures.csv (86423 filas)\n",
      "Procesando: poe_detail (subconjunto1)\n",
      "Guardado: subconjunto1_poe_detail.csv (8504982 filas)\n",
      "Procesando: prescriptions (subconjunto1)\n",
      "Guardado: subconjunto1_prescriptions.csv (20292611 filas)\n",
      "Procesando: omr (subconjunto1)\n",
      "Guardado: subconjunto1_omr.csv (7753027 filas)\n",
      "Procesando: services (subconjunto2)\n",
      "Guardado: subconjunto2_services.csv (593071 filas)\n",
      "Procesando: pharmacy (subconjunto2)\n",
      "Guardado: subconjunto2_pharmacy.csv (17847567 filas)\n",
      "Procesando: d_icd_diagnoses (subconjunto2)\n",
      "Guardado: subconjunto2_d_icd_diagnoses.csv (112107 filas)\n",
      "Procesando: diagnoses_icd (subconjunto2)\n",
      "Guardado: subconjunto2_diagnoses_icd.csv (6364488 filas)\n",
      "Procesando: emar_detail (subconjunto2)\n",
      "Guardado: subconjunto2_emar_detail.csv (87371064 filas)\n",
      "Procesando: labevents (subconjunto3)\n",
      "Guardado: subconjunto3_labevents.csv (158374764 filas)\n",
      "Procesando: d_labitems (subconjunto3)\n",
      "Guardado: subconjunto3_d_labitems.csv (1650 filas)\n",
      "Procesando: drgcodes (subconjunto3)\n",
      "Guardado: subconjunto3_drgcodes.csv (761856 filas)\n",
      "Procesando: emar (subconjunto3)\n",
      "Guardado: subconjunto3_emar.csv (42808593 filas)\n",
      "Procesando: microbiologyevents (subconjunto3)\n",
      "Guardado: subconjunto3_microbiologyevents.csv (3988224 filas)\n",
      "Procesando: transfers (subconjunto4)\n",
      "Guardado: subconjunto4_transfers.csv (2413581 filas)\n",
      "Procesando: d_hcpcs (subconjunto4)\n",
      "Guardado: subconjunto4_d_hcpcs.csv (89208 filas)\n",
      "Procesando: provider (subconjunto4)\n",
      "Guardado: subconjunto4_provider.csv (42244 filas)\n",
      "Procesando: procedures_icd (subconjunto4)\n",
      "Guardado: subconjunto4_procedures_icd.csv (859655 filas)\n",
      "Procesando: hcpcsevents (subconjunto4)\n",
      "Guardado: subconjunto4_hcpcsevents.csv (186074 filas)\n",
      "Procesando: admissions (subconjunto4)\n",
      "Guardado: subconjunto4_admissions.csv (546028 filas)\n",
      "Procesando: patients (subconjunto4)\n",
      "Guardado: subconjunto4_patients.csv (364627 filas)\n"
     ]
    }
   ],
   "source": [
    "# CARGAR DATOS EN SUBCONJUNTOS\n",
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_principal = r\"/home/raulmartinez/medicalReports/mimic-iv-3.1\"\n",
    "\n",
    "# Función para leer archivos y dividirlo en cuatro subconjuntos\n",
    "def cargar_y_dividir_datos(ruta_modulo):\n",
    "    archivos = [archivo for archivo in os.listdir(ruta_modulo) if archivo.endswith(\".gz\")]\n",
    "    total_archivos = len(archivos)\n",
    "    tam_subconjunto = total_archivos // 4\n",
    "\n",
    "    # División del conjunto hosp en 4 subconjuntos\n",
    "    for i, archivo in enumerate(archivos):\n",
    "        ruta_gz = os.path.join(ruta_modulo, archivo)\n",
    "        nombre_tabla = archivo.replace(\".csv.gz\", \"\")  # Nombre de la tabla sin la extensión\n",
    "\n",
    "        if i < tam_subconjunto:\n",
    "            parte = \"subconjunto1\"\n",
    "        elif i < 2*tam_subconjunto:\n",
    "            parte = \"subconjunto2\"\n",
    "        elif i < 3*tam_subconjunto:\n",
    "            parte = \"subconjunto3\"\n",
    "        else:\n",
    "            parte = \"subconjunto4\"\n",
    "        \n",
    "        print(f\"Procesando: {nombre_tabla} ({parte})\")\n",
    "        \n",
    "        # Leer el archivo comprimido\n",
    "        df = pd.read_csv(ruta_gz, compression=\"gzip\", low_memory=False)\n",
    "        # Guardar en un .csv nuevo\n",
    "        df.to_csv(f\"datasets/{parte}_{nombre_tabla}.csv\", index=False)\n",
    "        print(f\"Guardado: {parte}_{nombre_tabla}.csv ({df.shape[0]} filas)\")\n",
    "\n",
    "# Cargar datos de hosp\n",
    "ruta_hosp = os.path.join(ruta_principal, \"hosp\")\n",
    "print (\"Cargando datos de hosp...\")\n",
    "cargar_y_dividir_datos(ruta_hosp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesan los 4 subconjuntos: todos los archivos pertenecientes a cada subconjunto son concatenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: subconjunto1_prescriptionscsv\n",
      "Cargando: subconjunto1_poe_detailcsv\n",
      "Cargando: subconjunto1_poecsv\n",
      "Advertencia: subconjunto1_d_icd_procedurescsv no tiene 'subject_id'.\n",
      "Cargando: subconjunto1_omrcsv\n",
      "Dataset hosp unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Concatenacion subconjunto 1\n",
    "ruta_carpeta = \"datasets\"\n",
    "df_hosp = []\n",
    "\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.startswith(\"subconjunto1\"):\n",
    "        ruta_csv = os.path.join(ruta_carpeta, archivo)\n",
    "        df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "        if 'subject_id' in df.columns:\n",
    "            print(f\"Cargando: {archivo}\")\n",
    "            df_hosp.append(df)\n",
    "        else:\n",
    "            print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_hosp:\n",
    "    hosp_dataset = pd.concat(df_hosp, ignore_index=True)\n",
    "    hosp_dataset.to_csv(\"datasets/hosp_dataset_sub1.csv\", index=False)\n",
    "    print(\"Dataset hosp unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88762729, 34)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 1\n",
    "hosp_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advertencia: subconjunto2_d_icd_diagnosescsv no tiene 'subject_id'.\n",
      "Cargando: subconjunto2_pharmacycsv\n",
      "Cargando: subconjunto2_diagnoses_icdcsv\n",
      "Cargando: subconjunto2_servicescsv\n",
      "Cargando: subconjunto2_emar_detailcsv\n",
      "Dataset hosp unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Concatenacion subconjunto 2\n",
    "ruta_carpeta = \"datasets\"\n",
    "df_hosp = []\n",
    "\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.startswith(\"subconjunto2\"):\n",
    "        ruta_csv = os.path.join(ruta_carpeta, archivo)\n",
    "        df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "        if 'subject_id' in df.columns:\n",
    "            print(f\"Cargando: {archivo}\")\n",
    "            df_hosp.append(df)\n",
    "        else:\n",
    "            print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_hosp:\n",
    "    hosp_dataset = pd.concat(df_hosp, ignore_index=True)\n",
    "    hosp_dataset.to_csv(\"datasets/hosp_dataset_sub2.csv\", index=False)\n",
    "    print(\"Dataset hosp unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112176190, 63)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 2\n",
    "hosp_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: subconjunto3_drgcodescsv\n",
      "Cargando: subconjunto3_labeventscsv\n",
      "Advertencia: subconjunto3_d_labitemscsv no tiene 'subject_id'.\n",
      "Cargando: subconjunto3_emarcsv\n",
      "Cargando: subconjunto3_microbiologyeventscsv\n",
      "Dataset hosp unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Concatenacion subconjunto 3\n",
    "ruta_carpeta = \"datasets\"\n",
    "df_hosp = []\n",
    "\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.startswith(\"subconjunto3\"):\n",
    "        ruta_csv = os.path.join(ruta_carpeta, archivo)\n",
    "        df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "        if 'subject_id' in df.columns:\n",
    "            print(f\"Cargando: {archivo}\")\n",
    "            df_hosp.append(df)\n",
    "        else:\n",
    "            print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_hosp:\n",
    "    hosp_dataset = pd.concat(df_hosp, ignore_index=True)\n",
    "    hosp_dataset.to_csv(\"datasets/hosp_dataset_sub3.csv\", index=False)\n",
    "    print(\"Dataset hosp unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205933437, 48)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 3\n",
    "hosp_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: subconjunto4_hcpcseventscsv\n",
      "Advertencia: subconjunto4_providercsv no tiene 'subject_id'.\n",
      "Cargando: subconjunto4_admissionscsv\n",
      "Cargando: subconjunto4_patientscsv\n",
      "Cargando: subconjunto4_transferscsv\n",
      "Advertencia: subconjunto4_d_hcpcscsv no tiene 'subject_id'.\n",
      "Cargando: subconjunto4_procedures_icdcsv\n",
      "Dataset hosp unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Concatenacion subconjunto 4\n",
    "ruta_carpeta = \"datasets\"\n",
    "df_hosp = []\n",
    "\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.startswith(\"subconjunto4\"):\n",
    "        ruta_csv = os.path.join(ruta_carpeta, archivo)\n",
    "        df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "        if 'subject_id' in df.columns:\n",
    "            print(f\"Cargando: {archivo}\")\n",
    "            df_hosp.append(df)\n",
    "        else:\n",
    "            print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_hosp:\n",
    "    hosp_dataset = pd.concat(df_hosp, ignore_index=True)\n",
    "    hosp_dataset.to_csv(\"datasets/hosp_dataset_sub4.csv\", index=False)\n",
    "    print(\"Dataset hosp unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4369965, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 4\n",
    "hosp_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesar datos icu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, los archivos .csv comprimidos en la carpeta icu son descomprimidos y asignados a 4 subconjuntos, aunque más tarde esta asignación no será utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de icu...\n",
      "Procesando: procedureevents (subconjunto1)\n",
      "Guardado: subconjunto1_procedureevents.csv (808706 filas)\n",
      "Procesando: ingredientevents (subconjunto1)\n",
      "Guardado: subconjunto1_ingredientevents.csv (14253480 filas)\n",
      "Procesando: d_items (subconjunto2)\n",
      "Guardado: subconjunto2_d_items.csv (4095 filas)\n",
      "Procesando: icustays (subconjunto2)\n",
      "Guardado: subconjunto2_icustays.csv (94458 filas)\n",
      "Procesando: datetimeevents (subconjunto3)\n",
      "Guardado: subconjunto3_datetimeevents.csv (9979761 filas)\n",
      "Procesando: chartevents (subconjunto3)\n",
      "Guardado: subconjunto3_chartevents_part0.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part1.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part2.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part3.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part4.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part5.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part6.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part7.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part8.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part9.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part10.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part11.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part12.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part13.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part14.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part15.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part16.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part17.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part18.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part19.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part20.csv (20000000 filas)\n",
      "Guardado: subconjunto3_chartevents_part21.csv (12997491 filas)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_principal = r\"/home/raulmartinez/medicalReports/mimic-iv-3.1\"\n",
    "\n",
    "# Función para leer archivos y dividirlo en 4 subconjuntos\n",
    "def cargar_y_dividir_datos(ruta_modulo):\n",
    "    archivos = [archivo for archivo in os.listdir(ruta_modulo) if archivo.endswith(\".gz\")]\n",
    "    total_archivos = len(archivos)\n",
    "    tam_subconjunto = total_archivos // 4\n",
    "\n",
    "    for i, archivo in enumerate(archivos):\n",
    "        ruta_gz = os.path.join(ruta_modulo, archivo)\n",
    "        nombre_tabla = archivo.replace(\".csv.gz\", \"\")  # Nombre de la tabla sin la extensión\n",
    "\n",
    "        if i < tam_subconjunto:\n",
    "            parte = \"subconjunto1\"\n",
    "        elif i < 2*tam_subconjunto:\n",
    "            parte = \"subconjunto2\"\n",
    "        elif i < 3*tam_subconjunto:\n",
    "            parte = \"subconjunto3\"\n",
    "        else:\n",
    "            parte = \"subconjunto4\"\n",
    "        \n",
    "        print(f\"Procesando: {nombre_tabla} ({parte})\")\n",
    "\n",
    "        # Leer chartevents en chunks (3GB en total)\n",
    "        if archivo.startswith(\"chartevents\"):\n",
    "            # Leer en chunks y guardar en subconjuntos por problemas de RAM\n",
    "            for j, chunk in enumerate(pd.read_csv(ruta_gz, compression=\"gzip\", chunksize=20000000, low_memory=False)):\n",
    "                chunk.to_csv(f\"datasets/icu/{parte}_{nombre_tabla}_part{j}.csv\", index=False)\n",
    "                print(f\"Guardado: {parte}_{nombre_tabla}_part{j}.csv ({chunk.shape[0]} filas)\")\n",
    "        \n",
    "        df = pd.read_csv(ruta_gz, compression=\"gzip\", low_memory=False)\n",
    "        df.to_csv(f\"datasets/icu/{parte}_{nombre_tabla}.csv\", index=False)\n",
    "        print(f\"Guardado: {parte}_{nombre_tabla}.csv ({df.shape[0]} filas)\")\n",
    "\n",
    "# Cargar datos de icu\n",
    "ruta_icu = os.path.join(ruta_principal, \"icu\")\n",
    "print (\"Cargando datos de icu...\")\n",
    "cargar_y_dividir_datos(ruta_icu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aqui descomprimidos subconjunto1, 2 y 3. La siguiente celda preprocesa solo el subconjunto4, que por problemas de RAM se subdivide en más archivos que posteriormente seran concatenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de icu...\n",
      "Procesando: procedureevents (subconjunto1)\n",
      "Procesando: ingredientevents (subconjunto1)\n",
      "Procesando: d_items (subconjunto2)\n",
      "Procesando: icustays (subconjunto2)\n",
      "Procesando: datetimeevents (subconjunto3)\n",
      "Procesando: chartevents (subconjunto3)\n",
      "Procesando: inputevents (subconjunto4)\n",
      "Guardado: subconjunto4_inputevents.csv (10953713 filas)\n",
      "Procesando: caregiver (subconjunto4)\n",
      "Guardado: subconjunto4_caregiver.csv (17984 filas)\n",
      "Procesando: outputevents (subconjunto4)\n",
      "Guardado: subconjunto4_outputevents.csv (5359395 filas)\n"
     ]
    }
   ],
   "source": [
    "# CARGAR DATOS EN SUBCONJUNTOS\n",
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_principal = r\"/home/raulmartinez/medicalReports/mimic-iv-3.1\"\n",
    "\n",
    "# Función para leer archivos y dividirlo en dos subconjuntos\n",
    "def cargar_y_dividir_datos(ruta_modulo):\n",
    "    archivos = [archivo for archivo in os.listdir(ruta_modulo) if archivo.endswith(\".gz\")]\n",
    "    total_archivos = len(archivos)\n",
    "    tam_subconjunto = total_archivos // 4\n",
    "\n",
    "    for i, archivo in enumerate(archivos):\n",
    "        ruta_gz = os.path.join(ruta_modulo, archivo)\n",
    "        nombre_tabla = archivo.replace(\".csv.gz\", \"\")  # Nombre de la tabla sin la extensión\n",
    "\n",
    "        if i < tam_subconjunto:\n",
    "            parte = \"subconjunto1\"\n",
    "        elif i < 2*tam_subconjunto:\n",
    "            parte = \"subconjunto2\"\n",
    "        elif i < 3*tam_subconjunto:\n",
    "            parte = \"subconjunto3\"\n",
    "        else:\n",
    "            parte = \"subconjunto4\"\n",
    "        \n",
    "        print(f\"Procesando: {nombre_tabla} ({parte})\")\n",
    "\n",
    "        if parte == \"subconjunto4\":\n",
    "            df = pd.read_csv(ruta_gz, compression=\"gzip\", low_memory=False)\n",
    "            df.to_csv(f\"datasets/icu/{parte}_{nombre_tabla}.csv\", index=False)\n",
    "            print(f\"Guardado: {parte}_{nombre_tabla}.csv ({df.shape[0]} filas)\")\n",
    "\n",
    "# Cargar datos de hosp\n",
    "ruta_icu = os.path.join(ruta_principal, \"icu\")\n",
    "print (\"Cargando datos de icu...\")\n",
    "cargar_y_dividir_datos(ruta_icu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para concatenar los 4 subconjuntos de la carpeta icu, se crea la función que guarda las matrices de los nombres de los archivos de cada subconjunto. Esta si es la asignación de los subconjuntos que sí se va a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevos subconjuntos de icu\n",
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_datasets = r\"/home/raulmartinez/medicalReports/datasets/icu\"\n",
    "\n",
    "matriz_nombres_archivo_sub1 = []\n",
    "matriz_nombres_archivo_sub2 = []\n",
    "matriz_nombres_archivo_sub3 = []\n",
    "matriz_nombres_archivo_sub4 = []\n",
    "\n",
    "# Función para guardar las matrices de los nombres de los archivos de los 4 subconjuntos  de icu\n",
    "def calcular_nuevo_subconjunto(ruta_modulo):\n",
    "    archivos = [archivo for archivo in os.listdir(ruta_modulo) if archivo.endswith(\".csv\")]\n",
    "    total_archivos = len(archivos)\n",
    "    tam_subconjunto = total_archivos // 4\n",
    "\n",
    "    for i, archivo in enumerate(archivos):\n",
    "        ruta_gz = os.path.join(ruta_modulo, archivo)\n",
    "\n",
    "        if i < tam_subconjunto:\n",
    "            matriz_nombres_archivo_sub1.append(ruta_gz)\n",
    "        elif i < 2*tam_subconjunto:\n",
    "            matriz_nombres_archivo_sub2.append(ruta_gz)\n",
    "        elif i < 3*tam_subconjunto:\n",
    "            matriz_nombres_archivo_sub3.append(ruta_gz)\n",
    "        else:\n",
    "            matriz_nombres_archivo_sub4.append(ruta_gz)\n",
    "\n",
    "# Cargar datos de icu\n",
    "calcular_nuevo_subconjunto(ruta_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a unificar los 4 subconjuntos de icu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part21.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part3.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto4_outputevents.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part8.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part4.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part5.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part15.csv\n",
      "Dataset icu unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Unificando subconjunto 1 de icu\n",
    "df_icu = []\n",
    "\n",
    "for archivo in matriz_nombres_archivo_sub1:\n",
    "    df = pd.read_csv(archivo, low_memory=False)\n",
    "    if 'subject_id' in df.columns:\n",
    "        print(f\"Cargando: {archivo}\")\n",
    "        df_icu.append(df)\n",
    "    else:\n",
    "        print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_icu:\n",
    "    icu_dataset = pd.concat(df_icu, ignore_index=True)\n",
    "    icu_dataset.to_csv(\"datasets/icu_dataset_sub1.csv\", index=False)\n",
    "    print(\"Dataset icu unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part13.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto1_procedureevents.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto4_inputevents.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part20.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part16.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part19.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part18.csv\n",
      "Dataset icu unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Unificando subconjunto 2 de icu\n",
    "df_icu = []\n",
    "\n",
    "for archivo in matriz_nombres_archivo_sub2:\n",
    "    df = pd.read_csv(archivo, low_memory=False)\n",
    "    if 'subject_id' in df.columns:\n",
    "        print(f\"Cargando: {archivo}\")\n",
    "        df_icu.append(df)\n",
    "    else:\n",
    "        print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_icu:\n",
    "    icu_dataset = pd.concat(df_icu, ignore_index=True)\n",
    "    icu_dataset.to_csv(\"datasets/icu_dataset_sub2.csv\", index=False)\n",
    "    print(\"Dataset icu unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111762419, 33)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 2 de icu\n",
    "icu_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto1_ingredientevents.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part1.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part17.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part10.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto2_icustays.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part6.csv\n",
      "Advertencia: /home/raulmartinez/medicalReports/datasets/icu/subconjunto4_caregiver.csv no tiene 'subject_id'.\n",
      "Dataset icu unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Unificando subconjunto 3 de icu\n",
    "df_icu = []\n",
    "\n",
    "for archivo in matriz_nombres_archivo_sub3:\n",
    "    df = pd.read_csv(archivo, low_memory=False)\n",
    "    if 'subject_id' in df.columns:\n",
    "        print(f\"Cargando: {archivo}\")\n",
    "        df_icu.append(df)\n",
    "    else:\n",
    "        print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_icu:\n",
    "    icu_dataset = pd.concat(df_icu, ignore_index=True)\n",
    "    icu_dataset.to_csv(\"datasets/icu_dataset_sub3.csv\", index=False)\n",
    "    print(\"Dataset icu unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94347938, 27)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 3 de icu\n",
    "icu_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part7.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part2.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part0.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part14.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part12.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part11.csv\n",
      "Advertencia: /home/raulmartinez/medicalReports/datasets/icu/subconjunto2_d_items.csv no tiene 'subject_id'.\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_datetimeevents.csv\n",
      "Cargando: /home/raulmartinez/medicalReports/datasets/icu/subconjunto3_chartevents_part9.csv\n",
      "Dataset icu unificado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Unificando subconjunto 4 de icu\n",
    "df_icu = []\n",
    "\n",
    "for archivo in matriz_nombres_archivo_sub4:\n",
    "    df = pd.read_csv(archivo, low_memory=False)\n",
    "    if 'subject_id' in df.columns:\n",
    "        print(f\"Cargando: {archivo}\")\n",
    "        df_icu.append(df)\n",
    "    else:\n",
    "        print(f\"Advertencia: {archivo} no tiene 'subject_id'.\")\n",
    "\n",
    "if df_icu:\n",
    "    icu_dataset = pd.concat(df_icu, ignore_index=True)\n",
    "    icu_dataset.to_csv(\"datasets/icu_dataset_sub4.csv\", index=False)\n",
    "    print(\"Dataset icu unificado guardado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos válidos para unificar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149979761, 11)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de filas y columnas del subconjunto 4 de icu\n",
    "icu_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unir datasets hosp e icu con notes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset de las notas (notes) ha sido preprocesado en el archivo ``preprocessing_notes.ipynb`` . Ahí se ha ejecutado un código para guardar un fichero de texto que contiene todos los valores subject_id que contiene ese dataset. En esta parte lo que se hace es eliminar las filas de los subconjuntos de hosp e icu que contienen valores subject_id que no están presentes en el dataset notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta principal donde están las carpetas con los CSV\n",
    "ruta_principal = r\"/home/raulmartinez/medicalReports/datasets\"\n",
    "\n",
    "# Función para leer archivos y filtarlos para conservar solo los subject_id presentes en el texto\n",
    "def cargar_datos_subject_id(ruta_subconjunto, ruta_archivo_subject_id):\n",
    "    # Cargar csv\n",
    "    df = pd.read_csv(ruta_subconjunto, low_memory=False)\n",
    "    print(f\"Leido: {ruta_subconjunto} ({df.shape[0]} filas)\")\n",
    "\n",
    "    # Leer los subject_id desde el archivo de texto\n",
    "    with open(ruta_archivo_subject_id, \"r\") as f:\n",
    "        subject_ids = set(line.strip() for line in f)\n",
    "\n",
    "    # Filtrar el DataFrame para conservar solo los subject_id que están en el archivo de texto\n",
    "    df_filtrado = df[df['subject_id'].astype(str).isin(subject_ids)]\n",
    "\n",
    "    # Guardar el nuevo conjunto de datos\n",
    "    carpeta, archivo = os.path.split(ruta_subconjunto)\n",
    "    df_filtrado.to_csv(f\"datasets/union_hosp_icu_notes/{archivo}\", index=False)\n",
    "    print(f\"Guardado: datasets/union_hosp_icu_notes/{archivo}.csv ({df_filtrado.shape[0]} filas)\")\n",
    "    \n",
    "\n",
    "    # Mostrar los primeros resultados\n",
    "    print(df_filtrado.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por problemas de espacio se crea la función siguiente, que es igual que la anterior, solo que lee el subconjunto en varios chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer archivos y filtarlos para conservar solo los subject_id presentes en el texto\n",
    "def cargar_datos_subject_id_chunks(ruta_subconjunto, ruta_archivo_subject_id):\n",
    "\n",
    "    # Leer los subject_id desde el archivo de texto\n",
    "    with open(ruta_archivo_subject_id, \"r\") as f:\n",
    "        subject_ids = set(line.strip() for line in f)\n",
    "    \n",
    "    # Obtener el número total de filas para dividir en 8 chunks\n",
    "    total_rows = sum(1 for _ in open(ruta_subconjunto)) - 1  # Restar 1 por la cabecera\n",
    "    chunk_size = total_rows // 8  # Dividir en 8 partes\n",
    "\n",
    "    print(f\"Procesando: {ruta_subconjunto} ({total_rows} filas)\")\n",
    "    \n",
    "    carpeta, archivo = os.path.split(ruta_subconjunto)\n",
    "\n",
    "    # Leer el CSV en 8 chunks y filtrar cada uno\n",
    "    primer_chunk = True\n",
    "    for chunk in pd.read_csv(ruta_subconjunto, chunksize=chunk_size, low_memory=False):\n",
    "        chunk_filtrado = chunk[chunk['subject_id'].astype(str).isin(subject_ids)]\n",
    "        \n",
    "        # Guardar el nuevo conjunto de datos\n",
    "        chunk_filtrado.to_csv(f\"datasets/union_hosp_icu_notes/{archivo}\",mode='w' if primer_chunk else 'a', index=False, header=primer_chunk)\n",
    "        primer_chunk = False\n",
    "\n",
    "    \n",
    "    new_total_rows = sum(1 for _ in open(f\"datasets/union_hosp_icu_notes/{archivo}\")) - 1\n",
    "    print(f\"Guardado: datasets/union_hosp_icu_notes/{archivo} ({new_total_rows} filas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Leido: /home/raulmartinez/medicalReports/datasets/hosp/hosp_dataset_sub1.csv (88762729 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub1.csv.csv (74990340 filas)\n",
      "   subject_id     hadm_id  pharmacy_id       poe_id  poe_seq  \\\n",
      "0    10000032  22595853.0   12775705.0  10000032-55     55.0   \n",
      "1    10000032  22595853.0   18415984.0  10000032-42     42.0   \n",
      "2    10000032  22595853.0   23637373.0  10000032-35     35.0   \n",
      "3    10000032  22595853.0   26862314.0  10000032-41     41.0   \n",
      "4    10000032  22595853.0   30740602.0  10000032-27     27.0   \n",
      "\n",
      "  order_provider_id            starttime             stoptime drug_type  \\\n",
      "0            P85UQ1  2180-05-08 08:00:00  2180-05-07 22:00:00      MAIN   \n",
      "1            P23SJA  2180-05-07 02:00:00  2180-05-07 22:00:00      MAIN   \n",
      "2            P23SJA  2180-05-07 01:00:00  2180-05-07 09:00:00      MAIN   \n",
      "3            P23SJA  2180-05-07 01:00:00  2180-05-07 01:00:00      MAIN   \n",
      "4            P23SJA  2180-05-07 00:00:00  2180-05-07 22:00:00      MAIN   \n",
      "\n",
      "                          drug  ... order_type order_subtype  \\\n",
      "0                   Furosemide  ...        NaN           NaN   \n",
      "1      Ipratropium Bromide Neb  ...        NaN           NaN   \n",
      "2                   Furosemide  ...        NaN           NaN   \n",
      "3           Potassium Chloride  ...        NaN           NaN   \n",
      "4  Sodium Chloride 0.9%  Flush  ...        NaN           NaN   \n",
      "\n",
      "   transaction_type discontinue_of_poe_id discontinued_by_poe_id order_status  \\\n",
      "0               NaN                   NaN                    NaN          NaN   \n",
      "1               NaN                   NaN                    NaN          NaN   \n",
      "2               NaN                   NaN                    NaN          NaN   \n",
      "3               NaN                   NaN                    NaN          NaN   \n",
      "4               NaN                   NaN                    NaN          NaN   \n",
      "\n",
      "  chartdate seq_num result_name  result_value  \n",
      "0       NaN     NaN         NaN           NaN  \n",
      "1       NaN     NaN         NaN           NaN  \n",
      "2       NaN     NaN         NaN           NaN  \n",
      "3       NaN     NaN         NaN           NaN  \n",
      "4       NaN     NaN         NaN           NaN  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 1 de hosp\n",
    "ruta_csv = os.path.join(ruta_principal, \"hosp/hosp_dataset_sub1.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/hosp/hosp_dataset_sub2.csv (112176190 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv.csv (112176190 filas)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 2 de hosp\n",
    "ruta_csv = os.path.join(ruta_principal, \"hosp/hosp_dataset_sub2.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id_chunks(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv (89822760 filas)\n"
     ]
    }
   ],
   "source": [
    "# Total filas nuevo subconjunto 2\n",
    "new_total_rows = sum(1 for _ in open(f\"datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv\")) - 1\n",
    "print(f\"Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub2.csv ({new_total_rows} filas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/hosp/hosp_dataset_sub3.csv (205933437 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub3.csv (171703463 filas)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 3 de hosp\n",
    "ruta_csv = os.path.join(ruta_principal, \"hosp/hosp_dataset_sub3.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id_chunks(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Leido: /home/raulmartinez/medicalReports/datasets/hosp/hosp_dataset_sub4.csv (4369965 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/hosp_dataset_sub4.csv.csv (3634667 filas)\n",
      "   subject_id     hadm_id   chartdate hcpcs_cd  seq_num  \\\n",
      "0    10000068  25022803.0  2160-03-04    99218      1.0   \n",
      "1    10000084  29888819.0  2160-12-28    G0378      1.0   \n",
      "2    10000108  27250926.0  2163-09-27    99219      1.0   \n",
      "3    10000117  22927623.0  2181-11-15    43239      1.0   \n",
      "4    10000117  22927623.0  2181-11-15    G0378      2.0   \n",
      "\n",
      "               short_description admittime dischtime deathtime admission_type  \\\n",
      "0  Hospital observation services       NaN       NaN       NaN            NaN   \n",
      "1    Hospital observation per hr       NaN       NaN       NaN            NaN   \n",
      "2  Hospital observation services       NaN       NaN       NaN            NaN   \n",
      "3               Digestive system       NaN       NaN       NaN            NaN   \n",
      "4    Hospital observation per hr       NaN       NaN       NaN            NaN   \n",
      "\n",
      "   ... anchor_year anchor_year_group  dod transfer_id eventtype careunit  \\\n",
      "0  ...         NaN               NaN  NaN         NaN       NaN      NaN   \n",
      "1  ...         NaN               NaN  NaN         NaN       NaN      NaN   \n",
      "2  ...         NaN               NaN  NaN         NaN       NaN      NaN   \n",
      "3  ...         NaN               NaN  NaN         NaN       NaN      NaN   \n",
      "4  ...         NaN               NaN  NaN         NaN       NaN      NaN   \n",
      "\n",
      "  intime outtime icd_code  icd_version  \n",
      "0    NaN     NaN      NaN          NaN  \n",
      "1    NaN     NaN      NaN          NaN  \n",
      "2    NaN     NaN      NaN          NaN  \n",
      "3    NaN     NaN      NaN          NaN  \n",
      "4    NaN     NaN      NaN          NaN  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 4 de hosp\n",
    "ruta_csv = os.path.join(ruta_principal, \"hosp/hosp_dataset_sub4.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Leido: /home/raulmartinez/medicalReports/datasets/icu/icu_dataset_sub1.csv (118356886 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/icu_dataset_sub1.csv.csv (94840246 filas)\n",
      "   subject_id   hadm_id   stay_id  caregiver_id            charttime  \\\n",
      "0    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "1    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "2    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "3    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "4    19706392  27319499  33625056        8065.0  2140-09-25 11:00:00   \n",
      "\n",
      "             storetime  itemid value  valuenum  valueuom  warning  \n",
      "0  2140-09-25 11:14:00  220180    86      86.0      mmHg      0.0  \n",
      "1  2140-09-25 11:14:00  220181    93      93.0      mmHg      0.0  \n",
      "2  2140-09-25 11:14:00  220210    24      24.0  insp/min      0.0  \n",
      "3  2140-09-25 11:14:00  220277    99      99.0         %      0.0  \n",
      "4  2140-09-25 11:15:00  224650   NaN       NaN       NaN      0.0  \n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 1 de icu\n",
    "ruta_csv = os.path.join(ruta_principal, \"icu/icu_dataset_sub1.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/icu/icu_dataset_sub2.csv (111762419 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/icu_dataset_sub2.csv (89948910 filas)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 2 de icu\n",
    "ruta_csv = os.path.join(ruta_principal, \"icu/icu_dataset_sub2.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id_chunks(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/icu/icu_dataset_sub3.csv (94347938 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/icu_dataset_sub3.csv (77276054 filas)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 3 de icu\n",
    "ruta_csv = os.path.join(ruta_principal, \"icu/icu_dataset_sub3.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id_chunks(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Procesando: /home/raulmartinez/medicalReports/datasets/icu/icu_dataset_sub4.csv (149979761 filas)\n",
      "Guardado: datasets/union_hosp_icu_notes/icu_dataset_sub4.csv (124732175 filas)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos del subconjunto 4 de icu\n",
    "ruta_csv = os.path.join(ruta_principal, \"icu/icu_dataset_sub4.csv\")\n",
    "print (\"Cargando datos...\")\n",
    "cargar_datos_subject_id_chunks(ruta_csv, \"/home/raulmartinez/medicalReports/datasets/notes/subject_ids.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
