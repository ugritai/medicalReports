{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e45d1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77743779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los textos del servicio CARDIOTHORACIC\n",
    "df = pd.read_csv('./cardiothoracic_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cfa089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrer 150 textos de manera aleatoria\n",
    "df = df.sample(n=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b2d6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>admit_provider_id</th>\n",
       "      <th>admission_location</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>insurance</th>\n",
       "      <th>...</th>\n",
       "      <th>category_3</th>\n",
       "      <th>category_4</th>\n",
       "      <th>category_5</th>\n",
       "      <th>section_name</th>\n",
       "      <th>category_1_name</th>\n",
       "      <th>category_2_name</th>\n",
       "      <th>category_3_name</th>\n",
       "      <th>category_4_name</th>\n",
       "      <th>chapter_name</th>\n",
       "      <th>super_section_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>10224362</td>\n",
       "      <td>25595893</td>\n",
       "      <td>2157-04-12 00:00:00</td>\n",
       "      <td>2157-04-14 14:49:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SURGICAL SAME DAY ADMISSION</td>\n",
       "      <td>P16QV8</td>\n",
       "      <td>PHYSICIAN REFERRAL</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>...</td>\n",
       "      <td>R91.1</td>\n",
       "      <td>R91.1</td>\n",
       "      <td>R91.1</td>\n",
       "      <td>Abnormal findings on diagnostic imaging of lung</td>\n",
       "      <td>Solitary pulmonary nodule</td>\n",
       "      <td>Solitary pulmonary nodule</td>\n",
       "      <td>Solitary pulmonary nodule</td>\n",
       "      <td>Solitary pulmonary nodule</td>\n",
       "      <td>Symptoms, signs and abnormal clinical and labo...</td>\n",
       "      <td>Abnormal findings on diagnostic imaging and in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>10417511</td>\n",
       "      <td>28293295</td>\n",
       "      <td>2147-11-02 16:02:00</td>\n",
       "      <td>2147-11-07 13:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>URGENT</td>\n",
       "      <td>P33O7Z</td>\n",
       "      <td>TRANSFER FROM HOSPITAL</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Private</td>\n",
       "      <td>...</td>\n",
       "      <td>I25.10</td>\n",
       "      <td>I25.10</td>\n",
       "      <td>I25.10</td>\n",
       "      <td>Chronic ischemic heart disease</td>\n",
       "      <td>Atherosclerotic heart disease of native corona...</td>\n",
       "      <td>Atherosclerotic heart disease of native corona...</td>\n",
       "      <td>Atherosclerotic heart disease of native corona...</td>\n",
       "      <td>Atherosclerotic heart disease of native corona...</td>\n",
       "      <td>Diseases of the circulatory system</td>\n",
       "      <td>Ischemic heart diseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>10163774</td>\n",
       "      <td>25837438</td>\n",
       "      <td>2127-10-11 17:00:00</td>\n",
       "      <td>2127-10-13 10:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OBSERVATION ADMIT</td>\n",
       "      <td>P95BQY</td>\n",
       "      <td>EMERGENCY ROOM</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>...</td>\n",
       "      <td>R50.9</td>\n",
       "      <td>R50.9</td>\n",
       "      <td>R50.9</td>\n",
       "      <td>Fever of other and unknown origin</td>\n",
       "      <td>Fever, unspecified</td>\n",
       "      <td>Fever, unspecified</td>\n",
       "      <td>Fever, unspecified</td>\n",
       "      <td>Fever, unspecified</td>\n",
       "      <td>Symptoms, signs and abnormal clinical and labo...</td>\n",
       "      <td>General symptoms and signs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>10248673</td>\n",
       "      <td>28164505</td>\n",
       "      <td>2177-06-18 18:20:00</td>\n",
       "      <td>2177-06-25 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EW EMER.</td>\n",
       "      <td>P6512E</td>\n",
       "      <td>EMERGENCY ROOM</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>...</td>\n",
       "      <td>I25.10</td>\n",
       "      <td>I25.10</td>\n",
       "      <td>I25.10</td>\n",
       "      <td>Chronic ischemic heart disease</td>\n",
       "      <td>Atherosclerotic heart disease of native corona...</td>\n",
       "      <td>Atherosclerotic heart disease of native corona...</td>\n",
       "      <td>Atherosclerotic heart disease of native corona...</td>\n",
       "      <td>Atherosclerotic heart disease of native corona...</td>\n",
       "      <td>Diseases of the circulatory system</td>\n",
       "      <td>Ischemic heart diseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>10125966</td>\n",
       "      <td>29680994</td>\n",
       "      <td>2191-11-01 13:30:00</td>\n",
       "      <td>2191-11-05 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SURGICAL SAME DAY ADMISSION</td>\n",
       "      <td>P15314</td>\n",
       "      <td>PHYSICIAN REFERRAL</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>...</td>\n",
       "      <td>C34.30</td>\n",
       "      <td>C34.30</td>\n",
       "      <td>C34.30</td>\n",
       "      <td>Malignant neoplasm of bronchus and lung</td>\n",
       "      <td>Malignant neoplasm of lower lobe, bronchus or ...</td>\n",
       "      <td>Malignant neoplasm of lower lobe, unspecified ...</td>\n",
       "      <td>Malignant neoplasm of lower lobe, unspecified ...</td>\n",
       "      <td>Malignant neoplasm of lower lobe, unspecified ...</td>\n",
       "      <td>Neoplasms</td>\n",
       "      <td>Malignant neoplasms of respiratory and intrath...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject_id   hadm_id            admittime            dischtime deathtime  \\\n",
       "294    10224362  25595893  2157-04-12 00:00:00  2157-04-14 14:49:00       NaN   \n",
       "545    10417511  28293295  2147-11-02 16:02:00  2147-11-07 13:00:00       NaN   \n",
       "213    10163774  25837438  2127-10-11 17:00:00  2127-10-13 10:10:00       NaN   \n",
       "328    10248673  28164505  2177-06-18 18:20:00  2177-06-25 16:00:00       NaN   \n",
       "164    10125966  29680994  2191-11-01 13:30:00  2191-11-05 12:00:00       NaN   \n",
       "\n",
       "                  admission_type admit_provider_id      admission_location  \\\n",
       "294  SURGICAL SAME DAY ADMISSION            P16QV8      PHYSICIAN REFERRAL   \n",
       "545                       URGENT            P33O7Z  TRANSFER FROM HOSPITAL   \n",
       "213            OBSERVATION ADMIT            P95BQY          EMERGENCY ROOM   \n",
       "328                     EW EMER.            P6512E          EMERGENCY ROOM   \n",
       "164  SURGICAL SAME DAY ADMISSION            P15314      PHYSICIAN REFERRAL   \n",
       "\n",
       "    discharge_location insurance  ... category_3 category_4 category_5  \\\n",
       "294               HOME  Medicare  ...      R91.1      R91.1      R91.1   \n",
       "545   HOME HEALTH CARE   Private  ...     I25.10     I25.10     I25.10   \n",
       "213               HOME  Medicare  ...      R50.9      R50.9      R50.9   \n",
       "328   HOME HEALTH CARE  Medicare  ...     I25.10     I25.10     I25.10   \n",
       "164   HOME HEALTH CARE  Medicare  ...     C34.30     C34.30     C34.30   \n",
       "\n",
       "                                        section_name  \\\n",
       "294  Abnormal findings on diagnostic imaging of lung   \n",
       "545                   Chronic ischemic heart disease   \n",
       "213                Fever of other and unknown origin   \n",
       "328                   Chronic ischemic heart disease   \n",
       "164          Malignant neoplasm of bronchus and lung   \n",
       "\n",
       "                                       category_1_name  \\\n",
       "294                          Solitary pulmonary nodule   \n",
       "545  Atherosclerotic heart disease of native corona...   \n",
       "213                                 Fever, unspecified   \n",
       "328  Atherosclerotic heart disease of native corona...   \n",
       "164  Malignant neoplasm of lower lobe, bronchus or ...   \n",
       "\n",
       "                                       category_2_name  \\\n",
       "294                          Solitary pulmonary nodule   \n",
       "545  Atherosclerotic heart disease of native corona...   \n",
       "213                                 Fever, unspecified   \n",
       "328  Atherosclerotic heart disease of native corona...   \n",
       "164  Malignant neoplasm of lower lobe, unspecified ...   \n",
       "\n",
       "                                       category_3_name  \\\n",
       "294                          Solitary pulmonary nodule   \n",
       "545  Atherosclerotic heart disease of native corona...   \n",
       "213                                 Fever, unspecified   \n",
       "328  Atherosclerotic heart disease of native corona...   \n",
       "164  Malignant neoplasm of lower lobe, unspecified ...   \n",
       "\n",
       "                                       category_4_name  \\\n",
       "294                          Solitary pulmonary nodule   \n",
       "545  Atherosclerotic heart disease of native corona...   \n",
       "213                                 Fever, unspecified   \n",
       "328  Atherosclerotic heart disease of native corona...   \n",
       "164  Malignant neoplasm of lower lobe, unspecified ...   \n",
       "\n",
       "                                          chapter_name  \\\n",
       "294  Symptoms, signs and abnormal clinical and labo...   \n",
       "545                 Diseases of the circulatory system   \n",
       "213  Symptoms, signs and abnormal clinical and labo...   \n",
       "328                 Diseases of the circulatory system   \n",
       "164                                          Neoplasms   \n",
       "\n",
       "                                    super_section_name  \n",
       "294  Abnormal findings on diagnostic imaging and in...  \n",
       "545                            Ischemic heart diseases  \n",
       "213                         General symptoms and signs  \n",
       "328                            Ischemic heart diseases  \n",
       "164  Malignant neoplasms of respiratory and intrath...  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab96f1f",
   "metadata": {},
   "source": [
    "# Modelo gemma-2-2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903f1638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raulmartinez/pruebas_modelos/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Token de huggingface aÃ±adido en la terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbf84c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica si hay GPU disponible\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f032c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los textos del servicio CARDIOTHORACIC\n",
    "contexts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974061be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814964e",
   "metadata": {},
   "source": [
    "## Preguntas sin rol a temperatura por defecto (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876e6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_sin_rol(context, df):\n",
    "\n",
    "  chat = [\n",
    "    { \"role\": \"user\", \"content\": f\"Based on {context}. Give me a summary of the diagnostic for the patient\" },\n",
    "  ]\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "  inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "  outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "\n",
    "  decoded_output = tokenizer.decode(outputs[0])\n",
    "  #print(decoded_output)\n",
    "\n",
    "  # Decodificar solo la respuesta (sin el prompt)\n",
    "  generated_tokens = outputs[0][inputs.shape[-1]:]  # Recorta el prompt\n",
    "  decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "  # AÃ±adir las variables de entrada y salida para guardarlas en un df\n",
    "  new_row = {'input': prompt, 'output': decoded_output}\n",
    "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89be372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los textos del servicio CARDIOTHORACIC\n",
    "contexts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ef0a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gemma_sin_rol_1 = pd.DataFrame(columns=['input', 'output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5469af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_gemma_sin_rol_1 = pregunta_sin_rol(context, df_gemma_sin_rol_1)\n",
    "    print(f\"Procesadas {len(df_gemma_sin_rol_1)} respuestas\")\n",
    "    if( len(df_gemma_sin_rol_1) % 10 == 0):\n",
    "        df_gemma_sin_rol_1.to_csv('./analysis/gemma_sin_rol_1.csv')\n",
    "        print(f\"Guardadas {len(df_gemma_sin_rol_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb154ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nBased on  \\nName:  _...</td>\n",
       "      <td>## Summary of Diagnostic Findings:\\n\\nThis pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nBased on  \\nName:  _...</td>\n",
       "      <td>## Summary of Diagnostic Findings for Mr. ___\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nBased on  \\nName:  _...</td>\n",
       "      <td>This patient is a 68-year-old female with a hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nBased on  \\nName:  _...</td>\n",
       "      <td>This patient is a 70-year-old male with a hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nBased on  \\nName:  _...</td>\n",
       "      <td>## Summary of Diagnostic Findings:\\n\\nThis pat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <bos><start_of_turn>user\\nBased on  \\nName:  _...   \n",
       "1  <bos><start_of_turn>user\\nBased on  \\nName:  _...   \n",
       "2  <bos><start_of_turn>user\\nBased on  \\nName:  _...   \n",
       "3  <bos><start_of_turn>user\\nBased on  \\nName:  _...   \n",
       "4  <bos><start_of_turn>user\\nBased on  \\nName:  _...   \n",
       "\n",
       "                                              output  \n",
       "0  ## Summary of Diagnostic Findings:\\n\\nThis pat...  \n",
       "1  ## Summary of Diagnostic Findings for Mr. ___\\...  \n",
       "2  This patient is a 68-year-old female with a hi...  \n",
       "3  This patient is a 70-year-old male with a hist...  \n",
       "4  ## Summary of Diagnostic Findings:\\n\\nThis pat...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemma_sin_rol_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b24c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gemma_sin_rol_1.to_csv('./analysis/gemma_sin_rol_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "984e6469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemma_sin_rol_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b055a6c",
   "metadata": {},
   "source": [
    "## Preguntas con rol a temperatura por defecto (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1caf3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_con_rol(context, df):\n",
    "\n",
    "  chat = [\n",
    "    { \"role\": \"user\", \"content\": f\"Based on {context}. Give me a summary of the diagnostic for the patient\" },\n",
    "    { \"role\": \"expert\", \"content\": \"You are a professional medical doctor. You provide clear and concise summaries based on patient's clinical information\"}\n",
    "  ]\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "  inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "  outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "\n",
    "  decoded_output = tokenizer.decode(outputs[0])\n",
    "  #print(decoded_output)\n",
    "\n",
    "  # Decodificar solo la respuesta (sin el prompt)\n",
    "  generated_tokens = outputs[0][inputs.shape[-1]:]  # Recorta el prompt\n",
    "  decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "  # AÃ±adir las variables de entrada y salida para guardarlas en un df\n",
    "  new_row = {'input': prompt, 'output': decoded_output}\n",
    "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf651cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gemma_con_rol_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_gemma_con_rol_1 = pregunta_con_rol(context, df_gemma_con_rol_1)\n",
    "    print(f\"Procesadas {len(df_gemma_con_rol_1)} respuestas\")\n",
    "    if( len(df_gemma_con_rol_1) % 10 == 0):\n",
    "        df_gemma_con_rol_1.to_csv('./analysis/gemma_con_rol_1.csv')\n",
    "        print(f\"Guardadas {len(df_gemma_con_rol_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8faefb",
   "metadata": {},
   "source": [
    "## Preguntas sin rol a temperatura 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d1f8702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype,\n",
    "    temperature= 0.5,\n",
    "    do_sample=True)\n",
    "\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What do you know about Generative AI applied to medicine?\" },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a48706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gemma_sin_rol_0_5 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_gemma_sin_rol_0_5 = pregunta_sin_rol(context, df_gemma_sin_rol_0_5)\n",
    "    print(f\"Procesadas {len(df_gemma_sin_rol_0_5)} respuestas\")\n",
    "    if( len(df_gemma_sin_rol_0_5) % 10 == 0):\n",
    "        df_gemma_sin_rol_0_5.to_csv('./analysis/gemma_sin_rol_0_5.csv')\n",
    "        print(f\"Guardadas {len(df_gemma_sin_rol_0_5)} respuestas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aa4c8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemma_sin_rol_0_5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554beecc",
   "metadata": {},
   "source": [
    "## Preguntas con rol a temperatura 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ead0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gemma_con_rol_0_5 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_gemma_con_rol_0_5 = pregunta_con_rol(context, df_gemma_con_rol_0_5)\n",
    "    print(f\"Procesadas {len(df_gemma_con_rol_0_5)} respuestas\")\n",
    "    if( len(df_gemma_con_rol_0_5) % 10 == 0):\n",
    "        df_gemma_con_rol_0_5.to_csv('./analysis/gemma_con_rol_0_5.csv')\n",
    "        print(f\"Guardadas {len(df_gemma_con_rol_0_5)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea94c1a",
   "metadata": {},
   "source": [
    "## Preguntas sin rol a temperatura 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd174e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype,\n",
    "    temperature= 0.1,\n",
    "    do_sample=True)\n",
    "\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What do you know about Generative AI applied to medicine?\" },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gemma_sin_rol_0_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_gemma_sin_rol_0_1 = pregunta_sin_rol(context, df_gemma_sin_rol_0_1)\n",
    "    print(f\"Procesadas {len(df_gemma_sin_rol_0_1)} respuestas\")\n",
    "    if( len(df_gemma_sin_rol_0_1) % 10 == 0):\n",
    "        df_gemma_sin_rol_0_1.to_csv('./analysis/gemma_sin_rol_0_1.csv')\n",
    "        print(f\"Guardadas {len(df_gemma_sin_rol_0_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934dd75",
   "metadata": {},
   "source": [
    "## Preguntas con rol a 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gemma_con_rol_0_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_gemma_con_rol_0_1 = pregunta_con_rol(context, df_gemma_con_rol_0_1)\n",
    "    print(f\"Procesadas {len(df_gemma_con_rol_0_1)} respuestas\")\n",
    "    if( len(df_gemma_con_rol_0_1) % 10 == 0):\n",
    "        df_gemma_con_rol_0_1.to_csv('./analysis/gemma_con_rol_0_1.csv')\n",
    "        print(f\"Guardadas {len(df_gemma_con_rol_0_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767a21b",
   "metadata": {},
   "source": [
    "# Modelo Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a699c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Configura el pipeline con parÃ¡metros de generaciÃ³n\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    "    #top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e63821",
   "metadata": {},
   "source": [
    "## Preguntas sin rol a temperatura 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be124dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_sin_rol (context, df):\n",
    "  messages = [\n",
    "      { \"role\": \"user\", \"content\": f\"Based on {context}. Give me a summary of the diagnostic for the patient\" },\n",
    "  ]\n",
    "\n",
    "  # GeneraciÃ³n con parÃ¡metros controlados\n",
    "  outputs = pipe(\n",
    "      messages,\n",
    "      max_new_tokens=150,\n",
    "  )\n",
    "\n",
    "  # Procesamiento limpio de la salida\n",
    "  response = outputs[0]['generated_text'][-1]['content']\n",
    "  #print(response)\n",
    "\n",
    "  prompt = messages[0][\"content\"]\n",
    "  new_row = {'input': prompt, 'output': response}\n",
    "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llama_sin_rol_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_llama_sin_rol_1 = pregunta_sin_rol(context, df_llama_sin_rol_1)\n",
    "    print(f\"Procesadas {len(df_llama_sin_rol_1)} respuestas\")\n",
    "    if( len(df_llama_sin_rol_1) % 10 == 0):\n",
    "        df_llama_sin_rol_1.to_csv('./analysis/llama_sin_rol_1.csv')\n",
    "        print(f\"Guardadas {len(df_llama_sin_rol_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d990de",
   "metadata": {},
   "source": [
    "## Preguntas con rol a temperatura 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f2c4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_con_rol (context, df):\n",
    "  messages = [\n",
    "      { \"role\": \"user\", \"content\": f\"Based on {context}. Give me a summary of the diagnostic for the patient\" },\n",
    "      { \"role\": \"expert\", \"content\": \"You are a professional medical doctor. You provide clear and concise summaries based on patient's clinical information\"}\n",
    "  ]\n",
    "\n",
    "  # GeneraciÃ³n con parÃ¡metros controlados\n",
    "  outputs = pipe(\n",
    "      messages,\n",
    "      max_new_tokens=150,\n",
    "  )\n",
    "\n",
    "  # Procesamiento limpio de la salida\n",
    "  response = outputs[0]['generated_text'][-1]['content']\n",
    "  #print(response)\n",
    "\n",
    "  prompt = messages[0][\"content\"]\n",
    "  new_row = {'input': prompt, 'output': response}\n",
    "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfbb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llama_con_rol_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_llama_con_rol_1 = pregunta_con_rol(context, df_llama_con_rol_1)\n",
    "    print(f\"Procesadas {len(df_llama_con_rol_1)} respuestas\")\n",
    "    if( len(df_llama_con_rol_1) % 10 == 0):\n",
    "        df_llama_con_rol_1.to_csv('./analysis/llama_con_rol_1.csv')\n",
    "        print(f\"Guardadas {len(df_llama_con_rol_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b7e59",
   "metadata": {},
   "source": [
    "## Preguntas sin rol a temperatura 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1627bd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Configura el pipeline con parÃ¡metros de generaciÃ³n\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    #top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33769ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llama_sin_rol_0_5 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_llama_sin_rol_0_5 = pregunta_sin_rol(context, df_llama_sin_rol_0_5)\n",
    "    print(f\"Procesadas {len(df_llama_sin_rol_0_5)} respuestas\")\n",
    "    if( len(df_llama_sin_rol_0_5) % 10 == 0):\n",
    "        df_llama_sin_rol_0_5.to_csv('./analysis/llama_sin_rol_0_5.csv')\n",
    "        print(f\"Guardadas {len(df_llama_sin_rol_0_5)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a264ba",
   "metadata": {},
   "source": [
    "## Preguntas con rol a temperatura 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941696f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llama_con_rol_0_5 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_llama_con_rol_0_5 = pregunta_con_rol(context, df_llama_con_rol_0_5)\n",
    "    print(f\"Procesadas {len(df_llama_con_rol_0_5)} respuestas\")\n",
    "    if( len(df_llama_con_rol_0_5) % 10 == 0):\n",
    "        df_llama_con_rol_0_5.to_csv('./analysis/llama_con_rol_0_5.csv')\n",
    "        print(f\"Guardadas {len(df_llama_con_rol_0_5)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16284d87",
   "metadata": {},
   "source": [
    "## Preguntas sin rol a temperatura 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4680308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Configura el pipeline con parÃ¡metros de generaciÃ³n\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    #top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llama_sin_rol_0_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_llama_sin_rol_0_1 = pregunta_sin_rol(context, df_llama_sin_rol_0_1)\n",
    "    print(f\"Procesadas {len(df_llama_sin_rol_0_1)} respuestas\")\n",
    "    if( len(df_llama_sin_rol_0_1) % 10 == 0):\n",
    "        df_llama_sin_rol_0_1.to_csv('./analysis/llama_sin_rol_0_1.csv')\n",
    "        print(f\"Guardadas {len(df_llama_sin_rol_0_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea5147d",
   "metadata": {},
   "source": [
    "## Preguntas con rol a temperatura 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fcdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llama_con_rol_0_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_llama_con_rol_0_1 = pregunta_con_rol(context, df_llama_con_rol_0_1)\n",
    "    print(f\"Procesadas {len(df_llama_con_rol_0_1)} respuestas\")\n",
    "    if( len(df_llama_con_rol_0_1) % 10 == 0):\n",
    "        df_llama_con_rol_0_1.to_csv('./analysis/llama_con_rol_0_1.csv')\n",
    "        print(f\"Guardadas {len(df_llama_con_rol_0_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04d541",
   "metadata": {},
   "source": [
    "# Modelo Qwen3-4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c2659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bef4b",
   "metadata": {},
   "source": [
    "## Preguntas sin rol a temperatura 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9908cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_sin_rol(context, df):\n",
    "\n",
    "  messages = [\n",
    "    { \"role\": \"user\", \"content\": f\"Based on {context}. Give me a summary of the diagnostic for the patient\" },\n",
    "  ]\n",
    "\n",
    "  text = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True,\n",
    "      temperature=1.0,\n",
    "      enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "  )\n",
    "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  # conduct text completion\n",
    "  generated_ids = model.generate(\n",
    "      **model_inputs,\n",
    "      max_new_tokens=32768\n",
    "  )\n",
    "  output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "  # parsing thinking content\n",
    "  try:\n",
    "      # rindex finding 151668 (</think>)\n",
    "      index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "  except ValueError:\n",
    "      index = 0\n",
    "\n",
    "  #thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "  content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "  #print(\"thinking content:\", thinking_content)\n",
    "  #print(\"content:\", content)\n",
    "\n",
    "  prompt = messages[0][\"content\"]\n",
    "  new_row = {'input': prompt, 'output': content}\n",
    "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwen_sin_rol_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_qwen_sin_rol_1 = pregunta_sin_rol(context, df_qwen_sin_rol_1)\n",
    "    print(f\"Procesadas {len(df_qwen_sin_rol_1)} respuestas\")\n",
    "    if( len(df_qwen_sin_rol_1) % 10 == 0):\n",
    "        df_qwen_sin_rol_1.to_csv('./analysis/qwen_sin_rol_1.csv')\n",
    "        print(f\"Guardadas {len(df_qwen_sin_rol_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2d87e",
   "metadata": {},
   "source": [
    "## Preguntas con rol a temperatura 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1347b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_con_rol(context, df):\n",
    "\n",
    "  messages = [\n",
    "    { \"role\": \"user\", \"content\": f\"Based on {context}. Give me a summary of the diagnostic for the patient\" },\n",
    "    { \"role\": \"expert\", \"content\": \"You are a professional medical doctor. You provide clear and concise summaries based on patient's clinical information\"}\n",
    "  ]\n",
    "\n",
    "  text = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True,\n",
    "      enable_thinking=False, # Switches between thinking and non-thinking modes. Default is True.\n",
    "      temperature=1.0,\n",
    "      do_sample=True\n",
    "  )\n",
    "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  # conduct text completion\n",
    "  generated_ids = model.generate(\n",
    "      **model_inputs,\n",
    "      max_new_tokens=32768\n",
    "  )\n",
    "  output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "  # parsing thinking content\n",
    "  try:\n",
    "      # rindex finding 151668 (</think>)\n",
    "      index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "  except ValueError:\n",
    "      index = 0\n",
    "\n",
    "  #thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "  content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "  #print(\"thinking content:\", thinking_content)\n",
    "  #print(\"content:\", content)\n",
    "\n",
    "  prompt = messages[0][\"content\"]\n",
    "  new_row = {'input': prompt, 'output': content}\n",
    "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwen_con_rol_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_qwen_con_rol_1 = pregunta_con_rol(context, df_qwen_con_rol_1)\n",
    "    print(f\"Procesadas {len(df_qwen_con_rol_1)} respuestas\")\n",
    "    if( len(df_qwen_con_rol_1) % 10 == 0):\n",
    "        df_qwen_con_rol_1.to_csv('./analysis/qwen_con_rol_1.csv')\n",
    "        print(f\"Guardadas {len(df_qwen_con_rol_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300d16c",
   "metadata": {},
   "source": [
    "## Preguntas sin rol a temperatura 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91388e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_sin_rol(context, df):\n",
    "\n",
    "  messages = [\n",
    "    { \"role\": \"user\", \"content\": f\"Based on {context}. Give me a summary of the diagnostic for the patient\" },\n",
    "  ]\n",
    "\n",
    "  text = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True,\n",
    "      temperature=0.5,\n",
    "      enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "  )\n",
    "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  # conduct text completion\n",
    "  generated_ids = model.generate(\n",
    "      **model_inputs,\n",
    "      max_new_tokens=32768\n",
    "  )\n",
    "  output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "  # parsing thinking content\n",
    "  try:\n",
    "      # rindex finding 151668 (</think>)\n",
    "      index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "  except ValueError:\n",
    "      index = 0\n",
    "\n",
    "  #thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "  content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "  #print(\"thinking content:\", thinking_content)\n",
    "  #print(\"content:\", content)\n",
    "\n",
    "  prompt = messages[0][\"content\"]\n",
    "  new_row = {'input': prompt, 'output': content}\n",
    "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5507f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwen_sin_rol_0_5 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_qwen_sin_rol_0_5 = pregunta_sin_rol(context, df_qwen_sin_rol_0_5)\n",
    "    print(f\"Procesadas {len(df_qwen_sin_rol_0_5)} respuestas\")\n",
    "    if( len(df_qwen_sin_rol_0_5) % 10 == 0):\n",
    "        df_qwen_sin_rol_0_5.to_csv('./analysis/qwen_sin_rol_0_5.csv')\n",
    "        print(f\"Guardadas {len(df_qwen_sin_rol_0_5)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e800bbd",
   "metadata": {},
   "source": [
    "## Preguntas con rol a temperatura 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc2876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_con_rol(context, df):\n",
    "\n",
    "  messages = [\n",
    "    { \"role\": \"user\", \"content\": f\"Based on {context}. Give me a summary of the diagnostic for the patient\" },\n",
    "    { \"role\": \"expert\", \"content\": \"You are a professional medical doctor. You provide clear and concise summaries based on patient's clinical information\"}\n",
    "  ]\n",
    "\n",
    "  text = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True,\n",
    "      enable_thinking=False, # Switches between thinking and non-thinking modes. Default is True.\n",
    "      temperature=0.5,\n",
    "      do_sample=True\n",
    "  )\n",
    "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  # conduct text completion\n",
    "  generated_ids = model.generate(\n",
    "      **model_inputs,\n",
    "      max_new_tokens=32768\n",
    "  )\n",
    "  output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "  # parsing thinking content\n",
    "  try:\n",
    "      # rindex finding 151668 (</think>)\n",
    "      index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "  except ValueError:\n",
    "      index = 0\n",
    "\n",
    "  #thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "  content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "  #print(\"thinking content:\", thinking_content)\n",
    "  #print(\"content:\", content)\n",
    "\n",
    "  prompt = messages[0][\"content\"]\n",
    "  new_row = {'input': prompt, 'output': content}\n",
    "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwen_con_rol_0_5 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_qwen_con_rol_0_5 = pregunta_con_rol(context, df_qwen_con_rol_0_5)\n",
    "    print(f\"Procesadas {len(df_qwen_con_rol_0_5)} respuestas\")\n",
    "    if( len(df_qwen_con_rol_0_5) % 10 == 0):\n",
    "        df_qwen_con_rol_0_5.to_csv('./analysis/qwen_con_rol_0_5.csv')\n",
    "        print(f\"Guardadas {len(df_qwen_con_rol_0_5)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906246da",
   "metadata": {},
   "source": [
    "## Preguntas sin rol a temperatura 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a46c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_sin_rol(context, df):\n",
    "\n",
    "  messages = [\n",
    "    { \"role\": \"user\", \"content\": f\"Based on {context}. Give me a summary of the diagnostic for the patient\" },\n",
    "  ]\n",
    "\n",
    "  text = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True,\n",
    "      temperature=0.1,\n",
    "      enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "  )\n",
    "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  # conduct text completion\n",
    "  generated_ids = model.generate(\n",
    "      **model_inputs,\n",
    "      max_new_tokens=32768\n",
    "  )\n",
    "  output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "  # parsing thinking content\n",
    "  try:\n",
    "      # rindex finding 151668 (</think>)\n",
    "      index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "  except ValueError:\n",
    "      index = 0\n",
    "\n",
    "  #thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "  content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "  #print(\"thinking content:\", thinking_content)\n",
    "  #print(\"content:\", content)\n",
    "\n",
    "  prompt = messages[0][\"content\"]\n",
    "  new_row = {'input': prompt, 'output': content}\n",
    "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b458282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwen_sin_rol_0_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_qwen_sin_rol_0_1 = pregunta_sin_rol(context, df_qwen_sin_rol_0_1)\n",
    "    print(f\"Procesadas {len(df_qwen_sin_rol_0_1)} respuestas\")\n",
    "    if( len(df_qwen_sin_rol_0_1) % 10 == 0):\n",
    "        df_qwen_sin_rol_0_1.to_csv('./analysis/qwen_sin_rol_0_1.csv')\n",
    "        print(f\"Guardadas {len(df_qwen_sin_rol_0_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bde3b7",
   "metadata": {},
   "source": [
    "## Preguntas con rol a temperatura 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c77fa731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_con_rol(context, df):\n",
    "\n",
    "  messages = [\n",
    "    { \"role\": \"user\", \"content\": f\"Based on {context}. Give me a summary of the diagnostic for the patient\" },\n",
    "    { \"role\": \"expert\", \"content\": \"You are a professional medical doctor. You provide clear and concise summaries based on patient's clinical information\"}\n",
    "  ]\n",
    "\n",
    "  text = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True,\n",
    "      enable_thinking=False, # Switches between thinking and non-thinking modes. Default is True.\n",
    "      temperature=0.1,\n",
    "      do_sample=True\n",
    "  )\n",
    "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  # conduct text completion\n",
    "  generated_ids = model.generate(\n",
    "      **model_inputs,\n",
    "      max_new_tokens=32768\n",
    "  )\n",
    "  output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "  # parsing thinking content\n",
    "  try:\n",
    "      # rindex finding 151668 (</think>)\n",
    "      index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "  except ValueError:\n",
    "      index = 0\n",
    "\n",
    "  #thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "  content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "  #print(\"thinking content:\", thinking_content)\n",
    "  #print(\"content:\", content)\n",
    "\n",
    "  prompt = messages[0][\"content\"]\n",
    "  new_row = {'input': prompt, 'output': content}\n",
    "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qwen_con_rol_0_1 = pd.DataFrame(columns=['input', 'output'])\n",
    "\n",
    "# Obtener las respuestas del modelo para cada contexto\n",
    "for context in contexts:\n",
    "    df_qwen_con_rol_0_1 = pregunta_con_rol(context, df_qwen_con_rol_0_1)\n",
    "    print(f\"Procesadas {len(df_qwen_con_rol_0_1)} respuestas\")\n",
    "    if( len(df_qwen_con_rol_0_1) % 10 == 0):\n",
    "        df_qwen_con_rol_0_1.to_csv('./analysis/qwen_con_rol_0_1.csv')\n",
    "        print(f\"Guardadas {len(df_qwen_con_rol_0_1)} respuestas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b45b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen_con_rol_0_1.csv: (150, 3) (filas, columnas)\n",
      "llama_con_rol_0_1.csv: (150, 3) (filas, columnas)\n",
      "gemma_sin_rol_0_1.csv: (150, 3) (filas, columnas)\n",
      "llama_sin_rol_0_1.csv: (150, 3) (filas, columnas)\n",
      "qwen_con_rol_0_5.csv: (150, 3) (filas, columnas)\n",
      "gemma_sin_rol_0_5.csv: (150, 3) (filas, columnas)\n",
      "llama_con_rol_1.csv: (150, 3) (filas, columnas)\n",
      "gemma_con_rol_0_1.csv: (150, 3) (filas, columnas)\n",
      "llama_sin_rol_1.csv: (150, 3) (filas, columnas)\n",
      "llama_sin_rol_0_5.csv: (150, 3) (filas, columnas)\n",
      "gemma_sin_rol_1.csv: (150, 3) (filas, columnas)\n",
      "llama_con_rol_0_5.csv: (150, 3) (filas, columnas)\n",
      "qwen_sin_rol_0_5.csv: (150, 3) (filas, columnas)\n",
      "qwen_sin_rol_1.csv: (150, 3) (filas, columnas)\n",
      "gemma_con_rol_0_5.csv: (150, 3) (filas, columnas)\n",
      "qwen_con_rol_1.csv: (150, 4) (filas, columnas)\n",
      "gemma_con_rol_1.csv: (150, 3) (filas, columnas)\n",
      "qwen_sin_rol_0_1.csv: (150, 4) (filas, columnas)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = './analysis' \n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.startswith('gemma') or filename.startswith('llama') or filename.startswith('qwen'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"{filename}: {df.shape} (filas, columnas)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error leyendo {filename}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
